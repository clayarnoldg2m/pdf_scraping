{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd711a6-db4a-408a-b7e7-70331cc86e80",
   "metadata": {},
   "source": [
    "# PDF Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbebc25-e7ac-4797-8d11-e8df9ff9f08a",
   "metadata": {},
   "source": [
    "Copyright (c) 2025 Go2Market Insights, Inc\n",
    "All rights reserved.\n",
    "https://g2m.ai\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions\n",
    "of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\n",
    "THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
    "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e183a-9376-4314-83dd-5c3cd78b84ca",
   "metadata": {},
   "source": [
    "## Add Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2f33e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\clayarnold\\anaconda3\\lib\\site-packages (0.11.6)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in c:\\users\\clayarnold\\anaconda3\\lib\\site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\clayarnold\\anaconda3\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\clayarnold\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\clayarnold\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\clayarnold\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (43.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\clayarnold\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\clayarnold\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4825b2cf-199a-4efb-aa37-894140e2033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import textwrap\n",
    "from IPython.display import display\n",
    "import pdfplumber\n",
    "import re\n",
    "import typing\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from abc import ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85446d4-112d-41f3-aabc-e8e22fcc7676",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ad784a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://analyzr-llama-33-70b-test.eastus2.models.ai.azure.com/chat/completions\"\n",
    "key = \"o5Ko0yHozfM8DYg9ogQe7lsx0SUXhJtL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88fc47ed-73d4-4c44-8cb4-44f4006e6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST_TIMEOUT = 70 # in seconds\n",
    "LLM_TEMPERATURE = 0.5\n",
    "LLM_MAX_TOKENS = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38e4b3bd-e283-4c6e-a5f2-185534617806",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_ENDPOINTS = {\n",
    "    'llama3_1_small': {\n",
    "\t\t'best': {\n",
    "\t\t\t'url': url, # URL GOES HERE, \n",
    "            'key': key, # KEY GOES HERE,\n",
    "            'model_name': None, # Only necessary if model requires it (e.g. Some mistral models do require it)\n",
    "\t\t}, \n",
    "\t\t'fast': {\n",
    "            'url': None, # URL GOES HERE, \n",
    "            'key': None, # KEY GOES HERE,\n",
    "            'model_name': None, # Only necessary if model requires it (e.g. Some mistral models do require it)\n",
    "        }\n",
    "    }, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424b4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27b0d9c8-4429-49a1-bc26-0e0b7799785a",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9b2cf09-a7a6-4516-b9e0-415ee5617e8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# G2M PDFParser\n",
    "class g2mPDFParser:\n",
    "    \"\"\"\n",
    "    Class handling i/o with the LLM \n",
    "    \n",
    "    \"\"\"\n",
    "    # INITIALIZATION\n",
    "    def __init__(self, llm='llama3_1_small', query_type='best'):\n",
    "        \"\"\"\n",
    "        Initialize class instance \n",
    "\n",
    "        \"\"\"\n",
    "        match llm:\n",
    "            # case 'mistral' | 'mistral-nemo' | 'mistral2_2411' | 'mistral-small':\n",
    "            #     self.__llm = g2mLLMClientMistral(llm_type=llm, query_type=query_type)\n",
    "            case 'llama3_1_small' | 'llama3_1_large' | 'llama3_3':\n",
    "                self.__llm = g2mLLMClientLlama(llm_type=llm, query_type=query_type)\n",
    "            case _:\n",
    "                print('Unknown LLM type', f'llm_type={llm}')\n",
    "                self.__llm = None \n",
    "\n",
    "    # READ IN TEXT FILE\n",
    "    def read_in_text_file(self, file):\n",
    "        with open(file, \"r\") as file:\n",
    "            string = file.read()\n",
    "        return string\n",
    "\n",
    "    # GET FILE PATHS\n",
    "    def get_file_paths(folder_path):\n",
    "        file_paths = []\n",
    "        for root, directories, files in os.walk(folder_path):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                file_paths.append(file_path)\n",
    "        return file_paths\n",
    "        \n",
    "    # CONVERT TO TEXT\n",
    "    def convert_to_text(self, pdf, filepath=None, save=True):\n",
    "        with pdfplumber.open(pdf) as pdf2:\n",
    "            page = pdf2.pages[0]\n",
    "            text = page.extract_text()\n",
    "            print(pdf)\n",
    "            # print(text, \"\\n\")\n",
    "            #Save new text files\n",
    "            if save: \n",
    "                root, ext = os.path.splitext(pdf)\n",
    "                file = open(f'{root}-pdfplumber.txt',\"w\")\n",
    "                file.write(text)\n",
    "                file.close()\n",
    "    \n",
    "        return\n",
    "\n",
    "    # CONVERT PDF(s)\n",
    "    def convert_pdfs(self, files, filepath=None):\n",
    "        for file in files:\n",
    "            root, ext = os.path.splitext(file)\n",
    "            if ext == \".pdf\":\n",
    "                try: \n",
    "                    self.convert_to_text(file, 'C:\\\\filepath')\n",
    "                except: \n",
    "                    print(\"Warning! PDF could not be converted. \", file)\n",
    "\n",
    "    # BULK ANSWER AND SAVE\n",
    "    def bulk_answer_and_save(self, system='', files=None, save=False, filepath=None):\n",
    "        for file in files: \n",
    "            user = self.read_in_text_file(file)\n",
    "            res = self.query(user=user, system=system)\n",
    "            # Check if res is a dictionary and handle accordingly\n",
    "            if isinstance(res, dict):\n",
    "                if 'text' in res:\n",
    "                    text = res['text']\n",
    "                else:\n",
    "                    text = f\"Error: {res.get('message', 'Unknown error')}\"\n",
    "            else:\n",
    "                # This is the old code path, kept for backward compatibility\n",
    "                obj = json.loads(res.content)\n",
    "                text = obj['choices'][0]['text']\n",
    "                \n",
    "            print(text, \"\\n\")\n",
    "            if save: \n",
    "                root, ext = os.path.splitext(file)\n",
    "                if filepath is not None: \n",
    "                    file_path = f'{filepath}/{root}-LLM-answer.txt'\n",
    "                else:\n",
    "                    file_path = f'{root}-LLM-answer.txt'\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.write(text)\n",
    "        return\n",
    "            \n",
    "    # QUERY LLM\n",
    "    def query(self, user='', system='', temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS, query_type='best'):\n",
    "        \"\"\"\n",
    "        Send query to LLM. The user query and system context are provided separately.\n",
    "        The full prompt is assembled here using the appropriate syntax. \n",
    "\n",
    "        :param user: user query, e.g. 'hello, how are you'\n",
    "        :param system: system context and role, e.g. 'you are business analyst'\n",
    "        :param temperature:\n",
    "        :param max_tokens:\n",
    "        :param query_type:\n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        if self.__llm is not None:\n",
    "            res = self.__llm.query(user=user, system=system, temperature=temperature, max_tokens=max_tokens, query_type=query_type)\n",
    "        else:\n",
    "            print('LLM type unknown, aborting LLM query', type=self.__llm)\n",
    "            res = {'status': 'Unavailable', 'message': 'LLM type unknown'}\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "396a33b9-12f9-47b8-be4e-595a964a6c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# G2M LLM CLIENT BASE CLASS\n",
    "class g2mLLMClientBase(ABC):\n",
    "    \"\"\"\n",
    "    Class handling i/o with the LLM \n",
    "    \n",
    "    \"\"\"\n",
    "    def setLLm(self, query_type='best'): \n",
    "        \"\"\"\n",
    "        Set the appropriate LLM, especially based on query_type attribute (e.g. 'best' || 'fast')\n",
    "\n",
    "        :param query_type:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._url = LLM_ENDPOINTS[self._type][query_type]['url'] \n",
    "        self._api_key = LLM_ENDPOINTS[self._type][query_type]['key']\n",
    "        self._model_name = LLM_ENDPOINTS[self._type][query_type].get('model_name', None)\n",
    "        self._type = self._type\n",
    "        self._query_type = query_type\n",
    "\n",
    "    def query(self, user='', system='', temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS, query_type='best'):\n",
    "        \"\"\"\n",
    "        Send query to LLM. The user query and systen context are provided separately.\n",
    "        The full prompt is assembled here using the appropriate syntax. \n",
    "\n",
    "        :param user: user query, e.g. 'hello, how are you'\n",
    "        :param system: system context and role, e.g. 'you are business analyst'\n",
    "        :param temperature:\n",
    "        :param max_tokens:\n",
    "        :param query_type:\n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        self.setLLm(query_type=query_type)\n",
    "        body = {\n",
    "            'messages': [\n",
    "                {\n",
    "                    'role': 'system', \n",
    "                    'content': system, \n",
    "                }, \n",
    "                {\n",
    "                    'role': 'user', \n",
    "                    'content': user, \n",
    "                }, \n",
    "            ], \n",
    "            'temperature': temperature, \n",
    "            'max_tokens': max_tokens, \n",
    "        }\n",
    "        if self._model_name is not None: \n",
    "            print('Querying with model...', model_name=self._model_name)\n",
    "            body['model'] = self._model_name\n",
    "        return self._send_request(body)\n",
    "\n",
    "    def _send_request(self, body):\n",
    "        \"\"\"\n",
    "        Send JSON request to LLM API\n",
    "\n",
    "        :param body: \n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self._url is not None and self._api_key is not None:\n",
    "                res = requests.post(\n",
    "                    self._url,\n",
    "                    json=body,\n",
    "                    headers={\n",
    "                        \"Accept\": \"*/*\",\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"Authorization\": \"Bearer {}\".format(self._api_key),\n",
    "                    },\n",
    "                    timeout = REQUEST_TIMEOUT, \n",
    "                )\n",
    "                obj = json.loads(res.content)\n",
    "                text = self._parse_response(obj)\n",
    "                \n",
    "                # Comment out or remove the LLMObs section entirely\n",
    "                # LLMObs.annotate(\n",
    "                #     input_data=body['messages'],\n",
    "                #     output_data=[{\"role\": \"assistant\", \"content\": text}],\n",
    "                #     metadata={\n",
    "                #         \"temperature\": body['temperature'], \n",
    "                #         \"max_tokens\": body['temperature'], \n",
    "                #         \"llm_type\": self._type, \n",
    "                #     },\n",
    "                # )\n",
    "                    \n",
    "                res = {'status': 'Successful', 'text': text}\n",
    "            elif self._url is None:\n",
    "                print('LLM URL not specified, aborting LLM query', url=self._url)\n",
    "                res = {'status': 'Unavailable', 'message': 'URL not specified'}\n",
    "            else:\n",
    "                print('LLM access parameters not valid', url=self._url)\n",
    "                res = {'status': 'Unavailable', 'message': 'Invalid access parameters'}\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Cannot send LLM request: {}'.format(e), f'{self._url=}')\n",
    "            res = {'status': 'Unavailable', 'message': f'Cannot send request: {str(e)}'}\n",
    "        \n",
    "        return res  # Note: removed the extra except block that was shadowing the specific exception    \n",
    "    \n",
    "    def _parse_response(self, obj):\n",
    "        \"\"\"\n",
    "        Parse LLM response\n",
    "\n",
    "        :param obj:\n",
    "        :return text:\n",
    "        \"\"\"\n",
    "        # log.debug('Parsing response...', obj=obj)\n",
    "        if 'object' in obj.keys() and obj['object']=='Error':\n",
    "            text = obj['message']\n",
    "        elif 'error' in obj.keys(): \n",
    "            # Check if 'message' is a stringified JSON\n",
    "            if isinstance(obj['error']['message'], str):\n",
    "                try:\n",
    "                    error_message = json.loads(obj['error']['message'])\n",
    "                    print(f'[_parse_response] LLM response returned with error: {error_message}')\n",
    "                    text = 'Unable to give a response.'\n",
    "                except json.JSONDecodeError: \n",
    "                    text = obj['error']['message']\n",
    "            else:\n",
    "                text = obj['error']['message'].get('message', 'Unable to give a response.')\n",
    "        else:\n",
    "            text = obj['choices'][0]['message']['content'].strip()\n",
    "        return text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e993782b-d8ed-4b68-8447-823fa478541a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# G2M LLAMA LLM CLIENT\n",
    "class g2mLLMClientLlama(g2mLLMClientBase):\n",
    "    \"\"\"\n",
    "    Class handling i/o with the LLM \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm_type='llama3_1_small', query_type='best'):\n",
    "        \"\"\"\n",
    "        Initialize class instance \n",
    "\n",
    "        \"\"\"\n",
    "        self._url = LLM_ENDPOINTS[llm_type][query_type]['url'] \n",
    "        self._api_key = LLM_ENDPOINTS[llm_type][query_type]['key']\n",
    "        self._type = llm_type\n",
    "        self._query_type = query_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bafae4-4aac-44b7-bb26-f1e0923affba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CDP (formerly known as the Carbon Disclosure Project) Full Corporate Questionnaire is a comprehensive survey used to assess a company's environmental performance and sustainability practices. The questionnaire is divided into several modules, with Modules 1 to 6 covering the following topics:\n",
      "\n",
      "**Module 1: Climate Change**\n",
      "\n",
      "* Questions related to climate change governance, risk management, and strategy\n",
      "* Disclosure of greenhouse gas (GHG) emissions, emission reduction targets, and progress towards achieving them\n",
      "* Information on climate-related risks and opportunities, as well as the company's approach to managing them\n",
      "\n",
      "**Module 2: Risks and Opportunities**\n",
      "\n",
      "* Questions related to the identification, assessment, and management of climate-related risks and opportunities\n",
      "* Disclosure of potential climate-related impacts on the company's operations, supply chain, and revenue streams\n",
      "* Information on the company's approach to integrating climate-related considerations into its business strategy and decision-making processes\n",
      "\n",
      "**Module 3: Emissions**\n",
      "\n",
      "* Questions related to the company's GHG emissions, including scope 1, 2, and 3 emissions\n",
      "* Disclosure of emission reduction targets, as well as progress towards achieving them\n",
      "* Information on the company's approach to managing and reducing emissions, including the use of renewable energy and energy efficiency measures\n",
      "\n",
      "**Module 4: Energy**\n",
      "\n",
      "* Questions related to the company's energy use and management practices\n",
      "* Disclosure of energy consumption, energy efficiency measures, and renewable energy usage\n",
      "* Information on the company's approach to managing energy-related risks and opportunities\n",
      "\n",
      "**Module 5: Water**\n",
      "\n",
      "* Questions related to the company's water management practices and water-related risks\n",
      "* Disclosure of water usage, water efficiency measures, and water conservation efforts\n",
      "* Information on the company's approach to managing water-related risks and opportunities\n",
      "\n",
      "**Module 6: Supply Chain**\n",
      "\n",
      "* Questions related to the company's supply chain management practices and supply chain-related risks\n",
      "* Disclosure of supply chain GHG emissions, as well as efforts to reduce them\n",
      "* Information on the company's approach to managing supply chain-related risks and opportunities, including the use of sustainable procurement practices and supplier engagement.\n",
      "\n",
      "These modules provide a comprehensive framework for companies to disclose their environmental performance and sustainability practices, and for investors and stakeholders to assess their progress towards achieving environmental and sustainability goals. \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "prompt = \"you analyze documents\"\n",
    "files = [r'pdf_files\\CDP_2024_Corporate_Questionnaire_Guidance_Modules_1-6.pdf']\n",
    "\n",
    "# Example Use of G2M PDF Parser: \n",
    "parser = g2mPDFParser()\n",
    "res = parser.convert_pdfs(files)\n",
    "parser.bulk_answer_and_save(system=prompt, files=files, save=True)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb317c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b376b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
