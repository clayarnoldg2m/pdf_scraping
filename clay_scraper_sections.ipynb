{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64890fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1962c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample file (first 2 sections only)...\n",
      "Caching enabled\n",
      "Found 244 sections in pdf_files\\Jake_First_Output.txt\n",
      "Debug mode: Processing only first 2 of 244 sections\n",
      "Processing section 1/2: Prologue\n",
      "Processing section 2/2: (1.1)\n",
      "Results saved to pdf_files\\Jake_First_Output-qa-pairs.json\n",
      "Cache statistics: {'total_entries': 2, 'cache_size_bytes': 1397}\n",
      "\n",
      "Processing sample file again (all sections)...\n",
      "Caching enabled\n",
      "Found 244 sections in pdf_files\\Jake_First_Output.txt\n",
      "Processing section 1/244: Prologue\n",
      "Using cached LLM response\n",
      "Processing section 2/244: (1.1)\n",
      "Using cached LLM response\n",
      "Processing section 3/244: (1.2)\n",
      "Processing section 4/244: (1.3)\n",
      "Processing section 5/244: (1.4)\n",
      "Processing section 6/244: (1.5)\n",
      "Processing section 7/244: (1.6)\n",
      "Processing section 8/244: (1.7)\n",
      "Processing section 9/244: (1.8)\n",
      "Processing section 10/244: (1.8.1)\n",
      "Processing section 11/244: (1.9)\n",
      "Processing section 12/244: (1.10)\n",
      "Processing section 13/244: (1.11)\n",
      "Processing section 14/244: (1.12)\n",
      "Processing section 15/244: (1.13)\n",
      "Processing section 16/244: (1.14)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 767\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;66;03m# Process the same file again, should use cached responses for the first 2 sections\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing sample file again (all sections)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 767\u001b[0m output_file \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mprocess_file(input_file, use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# Print updated cache stats\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache statistics after second run:\u001b[39m\u001b[38;5;124m\"\u001b[39m, processor\u001b[38;5;241m.\u001b[39mget_cache_stats())\n",
      "Cell \u001b[1;32mIn[13], line 576\u001b[0m, in \u001b[0;36mSectionProcessor.process_file\u001b[1;34m(self, file_path, output_dir, max_sections, use_cache)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Extract questions and answers using LLM\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 576\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m    577\u001b[0m         user\u001b[38;5;241m=\u001b[39msection_text,\n\u001b[0;32m    578\u001b[0m         system\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt,\n\u001b[0;32m    579\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,  \u001b[38;5;66;03m# Lower temperature for more consistent outputs\u001b[39;00m\n\u001b[0;32m    580\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000\u001b[39m   \u001b[38;5;66;03m# Increased max tokens to ensure full answers\u001b[39;00m\n\u001b[0;32m    581\u001b[0m     )\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;66;03m# Process the response - extract JSON\u001b[39;00m\n\u001b[0;32m    585\u001b[0m         qa_text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[13], line 408\u001b[0m, in \u001b[0;36mg2mPDFParser.query\u001b[1;34m(self, user, system, temperature, max_tokens, query_type, use_cache)\u001b[0m\n\u001b[0;32m    405\u001b[0m should_use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m use_cache\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__llm\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m    409\u001b[0m         user\u001b[38;5;241m=\u001b[39muser, \n\u001b[0;32m    410\u001b[0m         system\u001b[38;5;241m=\u001b[39msystem, \n\u001b[0;32m    411\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature, \n\u001b[0;32m    412\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens, \n\u001b[0;32m    413\u001b[0m         query_type\u001b[38;5;241m=\u001b[39mquery_type,\n\u001b[0;32m    414\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39mshould_use_cache\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLM type unknown, aborting LLM query\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__llm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 160\u001b[0m, in \u001b[0;36mg2mLLMClientBase.query\u001b[1;34m(self, user, system, temperature, max_tokens, query_type, use_cache)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuerying with model...\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    158\u001b[0m     body[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name\n\u001b[1;32m--> 160\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(body)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Cache the response if caching is enabled\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuccessful\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[13], line 177\u001b[0m, in \u001b[0;36mg2mLLMClientBase._send_request\u001b[1;34m(self, body)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m         res \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url,\n\u001b[0;32m    179\u001b[0m             json\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    180\u001b[0m             headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    181\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*/*\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    182\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    183\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key),\n\u001b[0;32m    184\u001b[0m             },\n\u001b[0;32m    185\u001b[0m             timeout\u001b[38;5;241m=\u001b[39mREQUEST_TIMEOUT, \n\u001b[0;32m    186\u001b[0m         )\n\u001b[0;32m    187\u001b[0m         obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(res\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    188\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_response(obj)\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:730\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[0;32m    728\u001b[0m     server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 730\u001b[0m     sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[0;32m    731\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    732\u001b[0m         cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[0;32m    733\u001b[0m         ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[0;32m    734\u001b[0m         ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[0;32m    735\u001b[0m         ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[0;32m    736\u001b[0m         ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[0;32m    737\u001b[0m         ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[0;32m    738\u001b[0m         ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[0;32m    739\u001b[0m         cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[0;32m    740\u001b[0m         key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[0;32m    741\u001b[0m         key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[0;32m    742\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname_rm_dot,\n\u001b[0;32m    743\u001b[0m         ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[0;32m    744\u001b[0m         tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    745\u001b[0m         assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[0;32m    746\u001b[0m         assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[0;32m    747\u001b[0m     )\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:909\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[0;32m    907\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 909\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    910\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    911\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[0;32m    912\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[0;32m    913\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[0;32m    914\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[0;32m    915\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[0;32m    916\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[0;32m    917\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    918\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    919\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[0;32m    920\u001b[0m )\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\urllib3\\util\\ssl_.py:469\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    465\u001b[0m         context\u001b[38;5;241m.\u001b[39mload_cert_chain(certfile, keyfile, key_password)\n\u001b[0;32m    467\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 469\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\site-packages\\urllib3\\util\\ssl_.py:513\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    510\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    456\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    457\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    458\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    459\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    460\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    461\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    462\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    463\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   1039\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1041\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ClayArnold\\anaconda3\\Lib\\ssl.py:1319\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[0;32m   1318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pdfplumber\n",
    "import glob\n",
    "import hashlib\n",
    "from abc import ABC\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "url = \"https://analyzr-llama-33-70b-test.eastus2.models.ai.azure.com/chat/completions\"\n",
    "key = \"o5Ko0yHozfM8DYg9ogQe7lsx0SUXhJtL\"\n",
    "\n",
    "# Constants from your boilerplate\n",
    "REQUEST_TIMEOUT = 70  # in seconds\n",
    "LLM_TEMPERATURE = 0.5\n",
    "LLM_MAX_TOKENS = 3000\n",
    "LLM_ENDPOINTS = {\n",
    "    'llama3_3': {\n",
    "        'best': {\n",
    "            'url': url,  # URL GOES HERE\n",
    "            'key': key,  # KEY GOES HERE\n",
    "            'model_name': None,  # Only necessary if model requires it\n",
    "        },\n",
    "        'fast': {\n",
    "            'url': None,  # URL GOES HERE\n",
    "            'key': None,  # KEY GOES HERE\n",
    "            'model_name': None,  # Only necessary if model requires it\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Cache class for storing LLM responses\n",
    "class LLMResponseCache:\n",
    "    \"\"\"Cache for storing LLM responses to avoid repeated queries\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=\".cache\"):\n",
    "        \"\"\"Initialize the cache with a directory to store cache files\"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        self.cache = self._load_cache()\n",
    "    \n",
    "    def _generate_key(self, system, user, temperature, max_tokens):\n",
    "        \"\"\"Generate a unique key for the cache based on inputs\"\"\"\n",
    "        # Create a string with all parameters\n",
    "        key_string = f\"{system}|{user}|{temperature}|{max_tokens}\"\n",
    "        # Create a hash of this string for the key\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()\n",
    "    \n",
    "    def _get_cache_file_path(self):\n",
    "        \"\"\"Get the path to the cache file\"\"\"\n",
    "        return os.path.join(self.cache_dir, \"llm_response_cache.json\")\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load the cache from disk\"\"\"\n",
    "        cache_file = self._get_cache_file_path()\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cache: {str(e)}\")\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save the cache to disk\"\"\"\n",
    "        cache_file = self._get_cache_file_path()\n",
    "        try:\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cache: {str(e)}\")\n",
    "    \n",
    "    def get(self, system, user, temperature, max_tokens):\n",
    "        \"\"\"Get a response from the cache\"\"\"\n",
    "        key = self._generate_key(system, user, temperature, max_tokens)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def put(self, system, user, temperature, max_tokens, response):\n",
    "        \"\"\"Store a response in the cache\"\"\"\n",
    "        key = self._generate_key(system, user, temperature, max_tokens)\n",
    "        self.cache[key] = response\n",
    "        self._save_cache()\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the entire cache\"\"\"\n",
    "        self.cache = {}\n",
    "        self._save_cache()\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get statistics about the cache\"\"\"\n",
    "        return {\n",
    "            \"total_entries\": len(self.cache),\n",
    "            \"cache_size_bytes\": os.path.getsize(self._get_cache_file_path()) if os.path.exists(self._get_cache_file_path()) else 0\n",
    "        }\n",
    "\n",
    "# Keep your existing utility classes\n",
    "class g2mLLMClientBase(ABC):\n",
    "    \"\"\"Base class handling I/O with the LLM\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with cache\"\"\"\n",
    "        self.cache = LLMResponseCache()\n",
    "    \n",
    "    def setLLm(self, query_type='best'):\n",
    "        \"\"\"\n",
    "        Set the appropriate LLM, especially based on query_type attribute (e.g. 'best' || 'fast')\n",
    "\n",
    "        :param query_type:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._url = LLM_ENDPOINTS[self._type][query_type]['url'] \n",
    "        self._api_key = LLM_ENDPOINTS[self._type][query_type]['key']\n",
    "        self._model_name = LLM_ENDPOINTS[self._type][query_type].get('model_name', None)\n",
    "        self._type = self._type\n",
    "        self._query_type = query_type\n",
    "\n",
    "    def query(self, user='', system='', temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS, query_type='best', use_cache=True):\n",
    "        \"\"\"\n",
    "        Send query to LLM. The user query and system context are provided separately.\n",
    "        The full prompt is assembled here using the appropriate syntax. \n",
    "\n",
    "        :param user: user query, e.g. 'hello, how are you'\n",
    "        :param system: system context and role, e.g. 'you are business analyst'\n",
    "        :param temperature:\n",
    "        :param max_tokens:\n",
    "        :param query_type:\n",
    "        :param use_cache: whether to use the cache\n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        # Try to get from cache first if enabled\n",
    "        if use_cache:\n",
    "            cached_response = self.cache.get(system, user, temperature, max_tokens)\n",
    "            if cached_response:\n",
    "                print(\"Using cached LLM response\")\n",
    "                return cached_response\n",
    "        \n",
    "        # No cache hit, make a new request\n",
    "        self.setLLm(query_type=query_type)\n",
    "        body = {\n",
    "            'messages': [\n",
    "                {\n",
    "                    'role': 'system', \n",
    "                    'content': system, \n",
    "                }, \n",
    "                {\n",
    "                    'role': 'user', \n",
    "                    'content': user, \n",
    "                }, \n",
    "            ], \n",
    "            'temperature': temperature, \n",
    "            'max_tokens': max_tokens, \n",
    "        }\n",
    "        if self._model_name is not None:\n",
    "            print('Querying with model...', f'model_name={self._model_name}')\n",
    "            body['model'] = self._model_name\n",
    "        \n",
    "        response = self._send_request(body)\n",
    "        \n",
    "        # Cache the response if caching is enabled\n",
    "        if use_cache and response.get('status') == 'Successful':\n",
    "            self.cache.put(system, user, temperature, max_tokens, response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def _send_request(self, body):\n",
    "        \"\"\"\n",
    "        Send JSON request to LLM API\n",
    "\n",
    "        :param body: \n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self._url is not None and self._api_key is not None:\n",
    "                res = requests.post(\n",
    "                    self._url,\n",
    "                    json=body,\n",
    "                    headers={\n",
    "                        \"Accept\": \"*/*\",\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"Authorization\": \"Bearer {}\".format(self._api_key),\n",
    "                    },\n",
    "                    timeout=REQUEST_TIMEOUT, \n",
    "                )\n",
    "                obj = json.loads(res.content)\n",
    "                text = self._parse_response(obj)\n",
    "                \n",
    "                res = {'status': 'Successful', 'text': text}\n",
    "            elif self._url is None:\n",
    "                print('LLM URL not specified, aborting LLM query', f'url={self._url}')\n",
    "                res = {'status': 'Unavailable', 'message': 'URL not specified'}\n",
    "            else:\n",
    "                print('LLM access parameters not valid', f'url={self._url}')\n",
    "                res = {'status': 'Unavailable', 'message': 'Invalid access parameters'}\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Cannot send LLM request: {}'.format(e), f'url={self._url}')\n",
    "            res = {'status': 'Unavailable', 'message': f'Cannot send request: {str(e)}'}\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def _parse_response(self, obj):\n",
    "        \"\"\"\n",
    "        Parse LLM response\n",
    "\n",
    "        :param obj:\n",
    "        :return text:\n",
    "        \"\"\"\n",
    "        if 'object' in obj.keys() and obj['object'] == 'Error':\n",
    "            text = obj['message']\n",
    "        elif 'error' in obj.keys(): \n",
    "            # Check if 'message' is a stringified JSON\n",
    "            if isinstance(obj['error']['message'], str):\n",
    "                try:\n",
    "                    error_message = json.loads(obj['error']['message'])\n",
    "                    print(f'[_parse_response] LLM response returned with error: {error_message}')\n",
    "                    text = 'Unable to give a response.'\n",
    "                except json.JSONDecodeError: \n",
    "                    text = obj['error']['message']\n",
    "            else:\n",
    "                text = obj['error']['message'].get('message', 'Unable to give a response.')\n",
    "        else:\n",
    "            text = obj['choices'][0]['message']['content'].strip()\n",
    "        return text\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the response cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        print(\"Cache cleared\")\n",
    "\n",
    "class g2mLLMClientLlama(g2mLLMClientBase):\n",
    "    \"\"\"Class handling I/O with Llama LLM\"\"\"\n",
    "\n",
    "    def __init__(self, llm_type='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize class instance\"\"\"\n",
    "        super().__init__()  # Initialize the base class with cache\n",
    "        self._url = LLM_ENDPOINTS[llm_type][query_type]['url'] \n",
    "        self._api_key = LLM_ENDPOINTS[llm_type][query_type]['key']\n",
    "        self._type = llm_type\n",
    "        self._query_type = query_type\n",
    "\n",
    "class g2mPDFParser:\n",
    "    \"\"\"Class handling I/O with the LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, llm='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize class instance\"\"\"\n",
    "        match llm:\n",
    "            case 'llama3_1_small' | 'llama3_1_large' | 'llama3_3':\n",
    "                self.__llm = g2mLLMClientLlama(llm_type=llm, query_type=query_type)\n",
    "            case _:\n",
    "                print('Unknown LLM type', f'llm_type={llm}')\n",
    "                self.__llm = None \n",
    "        \n",
    "        # Default cache setting\n",
    "        self.use_cache = True\n",
    "\n",
    "    def read_in_text_file(self, file):\n",
    "        \"\"\"Read text from a file\"\"\"\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            string = f.read()\n",
    "        return string\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_paths(folder_path):\n",
    "        \"\"\"Get all file paths in a folder\"\"\"\n",
    "        file_paths = []\n",
    "        for root, directories, files in os.walk(folder_path):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                file_paths.append(file_path)\n",
    "        return file_paths\n",
    "        \n",
    "    def convert_to_text(self, pdf, filepath=None, save=True):\n",
    "        \"\"\"Convert PDF to text\"\"\"\n",
    "        with pdfplumber.open(pdf) as pdf2:\n",
    "            # Extract text from all pages, not just the first one\n",
    "            all_text = \"\"\n",
    "            for page in pdf2.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text += text + \"\\n\\n\"\n",
    "            \n",
    "            print(f\"Extracted {len(all_text)} characters from {pdf}\")\n",
    "            \n",
    "            # Save text to file\n",
    "            if save: \n",
    "                root, ext = os.path.splitext(pdf)\n",
    "                with open(f'{root}-pdfplumber.txt', \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(all_text)\n",
    "            \n",
    "        return all_text\n",
    "\n",
    "    def convert_pdfs(self, files, filepath=None):\n",
    "        \"\"\"Convert multiple PDFs to text\"\"\"\n",
    "        results = []\n",
    "        for file in files:\n",
    "            root, ext = os.path.splitext(file)\n",
    "            if ext.lower() == \".pdf\":\n",
    "                try: \n",
    "                    text = self.convert_to_text(file, filepath)\n",
    "                    results.append({\"file\": file, \"status\": \"success\", \"text\": text})\n",
    "                except Exception as e: \n",
    "                    print(f\"Warning! PDF could not be converted: {file}. Error: {str(e)}\")\n",
    "                    results.append({\"file\": file, \"status\": \"error\", \"message\": str(e)})\n",
    "        return results\n",
    "\n",
    "    def bulk_answer_and_save(self, system='', files=None, save=False, filepath=None, max_sections=None):\n",
    "        \"\"\"\n",
    "        Process multiple files with LLM\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process (for debugging)\n",
    "        \"\"\"\n",
    "        if files is None:\n",
    "            files = []\n",
    "            \n",
    "        results = []\n",
    "        for file in files: \n",
    "            # Handle the file extension\n",
    "            root, ext = os.path.splitext(file)\n",
    "            \n",
    "            try:\n",
    "                if ext.lower() == \".pdf\":\n",
    "                    # Convert PDF to text if needed\n",
    "                    text_file = f'{root}-pdfplumber.txt'\n",
    "                    if not os.path.exists(text_file):\n",
    "                        self.convert_to_text(file, filepath)\n",
    "                elif ext.lower() == \".txt\":\n",
    "                    # Already a text file\n",
    "                    text_file = file\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {ext}\")\n",
    "                    continue\n",
    "                \n",
    "                # Read the text content\n",
    "                user = self.read_in_text_file(text_file)\n",
    "                \n",
    "                # Process sections and query LLM\n",
    "                sections = self.split_text_into_sections(user)\n",
    "                all_questions = []\n",
    "                \n",
    "                # Apply section limit if specified\n",
    "                if max_sections is not None and max_sections > 0:\n",
    "                    print(f\"Debug mode: Processing only first {max_sections} of {len(sections)} sections\")\n",
    "                    sections = sections[:max_sections]\n",
    "                \n",
    "                for i, section in enumerate(sections):\n",
    "                    section_text, section_header = section\n",
    "                    print(f\"Processing section {i+1}/{len(sections)}: {section_header}\")\n",
    "                    \n",
    "                    # Query LLM for this section\n",
    "                    result = self.query(user=section_text, system=system, use_cache=self.use_cache)\n",
    "                    \n",
    "                    # Handle response\n",
    "                    if isinstance(result, dict) and 'text' in result:\n",
    "                        section_questions = result['text']\n",
    "                    else:\n",
    "                        try:\n",
    "                            obj = json.loads(result.content)\n",
    "                            section_questions = obj['choices'][0]['text']\n",
    "                        except:\n",
    "                            section_questions = f\"Error processing section {section_header}\"\n",
    "                    \n",
    "                    # Add to results\n",
    "                    all_questions.append({\n",
    "                        \"section\": section_header,\n",
    "                        \"questions\": section_questions\n",
    "                    })\n",
    "                    \n",
    "                # Save results if requested\n",
    "                if save:\n",
    "                    answer_file = f'{root}-questions.json'\n",
    "                    if filepath is not None:\n",
    "                        answer_file = os.path.join(filepath, os.path.basename(answer_file))\n",
    "                    \n",
    "                    with open(answer_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(all_questions, f, indent=2)\n",
    "                    \n",
    "                    print(f\"Saved questions to {answer_file}\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"file\": file,\n",
    "                    \"status\": \"success\",\n",
    "                    \"questions\": all_questions\n",
    "                })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "                results.append({\n",
    "                    \"file\": file,\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": str(e)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def query(self, user='', system='', temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS, query_type='best', use_cache=None):\n",
    "        \"\"\"\n",
    "        Query the LLM with optional cache control\n",
    "        \n",
    "        :param use_cache: Override instance cache setting\n",
    "        \"\"\"\n",
    "        # Determine whether to use cache\n",
    "        should_use_cache = self.use_cache if use_cache is None else use_cache\n",
    "        \n",
    "        if self.__llm is not None:\n",
    "            res = self.__llm.query(\n",
    "                user=user, \n",
    "                system=system, \n",
    "                temperature=temperature, \n",
    "                max_tokens=max_tokens, \n",
    "                query_type=query_type,\n",
    "                use_cache=should_use_cache\n",
    "            )\n",
    "        else:\n",
    "            print('LLM type unknown, aborting LLM query', f'type={self.__llm}')\n",
    "            res = {'status': 'Unavailable', 'message': 'LLM type unknown'}\n",
    "        return res\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the response cache\"\"\"\n",
    "        if self.__llm is not None:\n",
    "            self.__llm.clear_cache()\n",
    "    \n",
    "    def set_cache_enabled(self, enabled=True):\n",
    "        \"\"\"Set whether caching is enabled\"\"\"\n",
    "        self.use_cache = enabled\n",
    "        print(f\"Caching {'enabled' if enabled else 'disabled'}\")\n",
    "    \n",
    "    def split_text_into_sections(self, text):\n",
    "        \"\"\"\n",
    "        Split text into sections based on section headers like (1.2) or (1.2.3)\n",
    "        Returns a list of tuples: [(section_text, section_header), ...]\n",
    "        \"\"\"\n",
    "        # Pattern to match section headers like (1.2) or (1.2.3)\n",
    "        pattern = r'(\\(\\d+\\.\\d+(?:\\.\\d+)?\\))'\n",
    "        \n",
    "        # Find all matches\n",
    "        matches = list(re.finditer(pattern, text))\n",
    "        \n",
    "        sections = []\n",
    "        \n",
    "        # Process each match\n",
    "        for i in range(len(matches)):\n",
    "            # Get the current section header\n",
    "            header = matches[i].group(1)\n",
    "            \n",
    "            # Get the start of this section\n",
    "            start = matches[i].start()\n",
    "            \n",
    "            # Get the end of this section (start of next section or end of text)\n",
    "            if i < len(matches) - 1:\n",
    "                end = matches[i + 1].start()\n",
    "            else:\n",
    "                end = len(text)\n",
    "            \n",
    "            # Extract the section text\n",
    "            section_text = text[start:end].strip()\n",
    "            \n",
    "            # Add to our list of sections\n",
    "            sections.append((section_text, header))\n",
    "        \n",
    "        # If there's text before the first section, include it as a prologue\n",
    "        if matches and matches[0].start() > 0:\n",
    "            prologue = text[:matches[0].start()].strip()\n",
    "            if prologue:\n",
    "                sections.insert(0, (prologue, \"Prologue\"))\n",
    "        \n",
    "        # If no sections were found, return the entire text as one section\n",
    "        if not sections:\n",
    "            sections = [(text, \"Full Document\")]\n",
    "        \n",
    "        return sections\n",
    "\n",
    "class SectionProcessor:\n",
    "    \"\"\"Process text files by section and extract questions and answers using LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_type='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize with LLM client\"\"\"\n",
    "        self.parser = g2mPDFParser(llm=llm_type, query_type=query_type)\n",
    "        # Updated system prompt to extract both questions and answers\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert at extracting questions and their answers from text content.\n",
    "        \n",
    "        Your task is to analyze the given section of text and identify any explicit or implicit questions it contains,\n",
    "        along with their corresponding answers from the text.\n",
    "        \n",
    "        For each question you identify:\n",
    "        1. Extract or formulate the complete question\n",
    "        2. Extract or formulate the complete answer based on the text\n",
    "        3. Ensure both question and answer make sense on their own without needing additional context\n",
    "        4. Preserve the original meaning and intent\n",
    "        \n",
    "        Return your response in JSON format as follows:\n",
    "        ```json\n",
    "        {\n",
    "          \"qa_pairs\": [\n",
    "            {\n",
    "              \"question\": \"The full question text\",\n",
    "              \"answer\": \"The full answer text from the document\"\n",
    "            },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "        ```\n",
    "        \n",
    "        Important guidelines:\n",
    "        - Focus on extracting questions that are seeking specific information\n",
    "        - If the text contains incomplete questions, formulate them into complete questions\n",
    "        - If answers are implied rather than explicitly stated, formulate them based on the text\n",
    "        - Avoid creating questions or answers that weren't implied in the original text\n",
    "        - Maintain the technical terminology and specificity of the original content\n",
    "        - Include ALL text that constitutes the answer - don't truncate\n",
    "        \"\"\"\n",
    "    \n",
    "    def process_file(self, file_path, output_dir=None, max_sections=None, use_cache=True):\n",
    "        \"\"\"\n",
    "        Process a single text file and extract questions and answers by section\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process (for debugging)\n",
    "        :param use_cache: Whether to use the LLM response cache\n",
    "        \"\"\"\n",
    "        # Set cache configuration\n",
    "        self.parser.set_cache_enabled(use_cache)\n",
    "        \n",
    "        # Determine output directory\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.dirname(file_path)\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate output filename\n",
    "        base_name = os.path.basename(file_path)\n",
    "        name_without_ext = os.path.splitext(base_name)[0]\n",
    "        output_file = os.path.join(output_dir, f\"{name_without_ext}-qa-pairs.json\")\n",
    "        \n",
    "        # Read the file\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            # Convert PDF to text first\n",
    "            text = self.parser.convert_to_text(file_path, save=True)\n",
    "            if not text:\n",
    "                print(f\"Error: Could not extract text from PDF {file_path}\")\n",
    "                return None\n",
    "        else:\n",
    "            # Read text file directly\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "                return None\n",
    "        \n",
    "        # Split into sections\n",
    "        sections = self.parser.split_text_into_sections(text)\n",
    "        print(f\"Found {len(sections)} sections in {file_path}\")\n",
    "        \n",
    "        # Apply section limit if specified\n",
    "        if max_sections is not None and max_sections > 0:\n",
    "            print(f\"Debug mode: Processing only first {max_sections} of {len(sections)} sections\")\n",
    "            sections = sections[:max_sections]\n",
    "        \n",
    "        # Process each section\n",
    "        results = []\n",
    "        for i, (section_text, section_header) in enumerate(sections):\n",
    "            print(f\"Processing section {i+1}/{len(sections) if max_sections is None else max_sections}: {section_header}\")\n",
    "            \n",
    "            # Skip very short sections\n",
    "            if len(section_text.strip()) < 50:\n",
    "                print(f\"Skipping section {section_header} - too short\")\n",
    "                continue\n",
    "            \n",
    "            # Extract questions and answers using LLM\n",
    "            try:\n",
    "                response = self.parser.query(\n",
    "                    user=section_text,\n",
    "                    system=self.system_prompt,\n",
    "                    temperature=0.3,  # Lower temperature for more consistent outputs\n",
    "                    max_tokens=4000   # Increased max tokens to ensure full answers\n",
    "                )\n",
    "                \n",
    "                if isinstance(response, dict) and 'text' in response:\n",
    "                    # Process the response - extract JSON\n",
    "                    qa_text = response['text']\n",
    "                    \n",
    "                    # Extract JSON from the response\n",
    "                    try:\n",
    "                        # Look for JSON content between ```json and ``` if present\n",
    "                        json_match = re.search(r'```json\\s*(.*?)\\s*```', qa_text, re.DOTALL)\n",
    "                        if json_match:\n",
    "                            qa_json = json.loads(json_match.group(1))\n",
    "                        else:\n",
    "                            # Try parsing the entire response as JSON\n",
    "                            qa_json = json.loads(qa_text)\n",
    "                        \n",
    "                        # Add to results\n",
    "                        results.append({\n",
    "                            \"section_id\": section_header,\n",
    "                            \"section_text\": section_text,  # Store the full section text\n",
    "                            \"qa_pairs\": qa_json.get(\"qa_pairs\", [])\n",
    "                        })\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON response for section {section_header}: {str(e)}\")\n",
    "                        print(f\"Raw response: {qa_text[:100]}...\")\n",
    "                        results.append({\n",
    "                            \"section_id\": section_header,\n",
    "                            \"error\": \"JSON parsing error\",\n",
    "                            \"raw_response\": qa_text\n",
    "                        })\n",
    "                else:\n",
    "                    print(f\"Error: Unexpected response format for section {section_header}\")\n",
    "                    results.append({\n",
    "                        \"section_id\": section_header,\n",
    "                        \"error\": \"Unexpected response format\",\n",
    "                        \"raw_response\": str(response)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing section {section_header}: {str(e)}\")\n",
    "                results.append({\n",
    "                    \"section_id\": section_header,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Save results to JSON file\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    \"file\": file_path,\n",
    "                    \"total_sections\": len(sections),\n",
    "                    \"processed_sections\": len(results),\n",
    "                    \"results\": results\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"Results saved to {output_file}\")\n",
    "            return output_file\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def process_directory(self, directory_path, output_dir=None, max_sections=None, use_cache=True):\n",
    "        \"\"\"\n",
    "        Process all text and PDF files in a directory\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process per file (for debugging)\n",
    "        :param use_cache: Whether to use the LLM response cache\n",
    "        \"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.join(directory_path, \"questions_output\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Get all text and PDF files\n",
    "        files = []\n",
    "        for extension in ['.txt', '.pdf']:\n",
    "            files.extend(glob.glob(os.path.join(directory_path, f\"*{extension}\")))\n",
    "        \n",
    "        results = []\n",
    "        for file in files:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            output_file = self.process_file(\n",
    "                file, \n",
    "                output_dir, \n",
    "                max_sections=max_sections,\n",
    "                use_cache=use_cache\n",
    "            )\n",
    "            if output_file:\n",
    "                results.append({\n",
    "                    \"input_file\": file,\n",
    "                    \"output_file\": output_file,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "            else:\n",
    "                results.append({\n",
    "                    \"input_file\": file,\n",
    "                    \"status\": \"error\"\n",
    "                })\n",
    "        \n",
    "        # Save summary\n",
    "        summary_file = os.path.join(output_dir, \"processing_summary.json\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"directory\": directory_path,\n",
    "                \"files_processed\": len(files),\n",
    "                \"successful\": sum(1 for r in results if r[\"status\"] == \"success\"),\n",
    "                \"failed\": sum(1 for r in results if r[\"status\"] == \"error\"),\n",
    "                \"details\": results\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        return summary_file\n",
    "    \n",
    "    def clean_questions_list(self, text):\n",
    "        \"\"\"Clean up the list of questions from LLM output\"\"\"\n",
    "        # Split by newlines first\n",
    "        lines = text.strip().split('\\n')\n",
    "        \n",
    "        # Clean list of questions\n",
    "        questions = []\n",
    "        for line in lines:\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            # Remove numbering and leading/trailing whitespace\n",
    "            # Match patterns like \"1.\", \"1)\", \"Question 1:\", etc.\n",
    "            cleaned = re.sub(r'^\\s*(\\d+[\\.\\):]|Question\\s+\\d+:?)\\s*', '', line).strip()\n",
    "            \n",
    "            if cleaned:\n",
    "                questions.append(cleaned)\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the LLM response cache\"\"\"\n",
    "        self.parser.clear_cache()\n",
    "        print(\"Cache cleared\")\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get statistics about the cache\"\"\"\n",
    "        if hasattr(self.parser, '_g2mPDFParser__llm') and self.parser._g2mPDFParser__llm is not None:\n",
    "            return self.parser._g2mPDFParser__llm.cache.get_cache_stats()\n",
    "        return {\"error\": \"LLM client not properly initialized\"}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the processor\n",
    "    processor = SectionProcessor(llm_type='llama3_3', query_type='best')\n",
    "    \n",
    "    # Create a sample text file for testing\n",
    "    input_file = r\"pdf_files\\Jake_First_Output.txt\"\n",
    "    # with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(\"\"\"\n",
    "    #     Introduction to the document\n",
    "        \n",
    "    #     (1.1) First Section\n",
    "    #     This is content of the first section which discusses important concepts.\n",
    "    #     What are the key areas to focus on for this topic?\n",
    "    #     The document highlights several approaches to consider when implementing these ideas.\n",
    "        \n",
    "    #     (1.2) Second Section\n",
    "    #     In this section, we examine the relationship between various factors.\n",
    "    #     How do these factors influence outcomes?\n",
    "    #     It's important to understand the implications for practical applications.\n",
    "        \n",
    "    #     (2.1) Another Major Section\n",
    "    #     This section introduces new methodologies and frameworks.\n",
    "    #     Which framework is most appropriate for different scenarios?\n",
    "    #     Consider how these methodologies can be adapted to specific contexts.\n",
    "        \n",
    "    #     (2.1.1) Subsection with more detail\n",
    "    #     Here we dive deeper into specific aspects of the framework.\n",
    "    #     What are the limitations of this approach?\n",
    "    #     Several case studies demonstrate successful implementation.\n",
    "    #     \"\"\")\n",
    "    \n",
    "    # Process the sample file with caching enabled and limiting to 2 sections\n",
    "    print(\"Processing sample file (first 2 sections only)...\")\n",
    "    output_file = processor.process_file(input_file, max_sections=2, use_cache=True)\n",
    "    \n",
    "    # Print cache stats\n",
    "    print(\"Cache statistics:\", processor.get_cache_stats())\n",
    "    \n",
    "    # Process the same file again, should use cached responses for the first 2 sections\n",
    "    print(\"\\nProcessing sample file again (all sections)...\")\n",
    "    output_file = processor.process_file(input_file, use_cache=True)\n",
    "    \n",
    "    # Print updated cache stats\n",
    "    print(\"Cache statistics after second run:\", processor.get_cache_stats())\n",
    "    \n",
    "    # Clean up sample file\n",
    "    # os.remove(input_file)\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Results saved to {output_file}\")\n",
    "    \n",
    "    # Example of how to clear the cache if needed\n",
    "    processor.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b396b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bbdab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
