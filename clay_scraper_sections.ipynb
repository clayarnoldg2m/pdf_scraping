{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64890fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1962c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample file (first 2 sections only)...\n",
      "Caching enabled\n",
      "Found 5 sections in pdf_files\\CDP_2024_Corporate_Questionnaire_Guidance_Modules_1-6-debug-extraction.txt\n",
      "Debug mode: Processing only first 2 of 5 sections\n",
      "Processing section 1/2: Prologue\n",
      "Skipping section Prologue - too short\n",
      "Processing section 2/2: (1.1)\n",
      "Using cached LLM response\n",
      "Results saved to pdf_files\\CDP_2024_Corporate_Questionnaire_Guidance_Modules_1-6-debug-extraction-questions.json\n",
      "Cache statistics: {'total_entries': 4, 'cache_size_bytes': 566}\n",
      "\n",
      "Processing sample file again (all sections)...\n",
      "Caching enabled\n",
      "Found 5 sections in pdf_files\\CDP_2024_Corporate_Questionnaire_Guidance_Modules_1-6-debug-extraction.txt\n",
      "Processing section 1/5: Prologue\n",
      "Skipping section Prologue - too short\n",
      "Processing section 2/5: (1.1)\n",
      "Using cached LLM response\n",
      "Processing section 3/5: (1.2)\n",
      "Using cached LLM response\n",
      "Processing section 4/5: (2.1)\n",
      "Using cached LLM response\n",
      "Processing section 5/5: (2.1.1)\n",
      "Using cached LLM response\n",
      "Results saved to pdf_files\\CDP_2024_Corporate_Questionnaire_Guidance_Modules_1-6-debug-extraction-questions.json\n",
      "Cache statistics after second run: {'total_entries': 4, 'cache_size_bytes': 566}\n",
      "\n",
      "Processing complete. Results saved to pdf_files\\CDP_2024_Corporate_Questionnaire_Guidance_Modules_1-6-debug-extraction-questions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:700: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:700: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\ClayArnold\\AppData\\Local\\Temp\\ipykernel_124320\\3125150069.py:700: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  input_file = \"pdf_files\\CDP_2024_Corporate_Questionnaire_Guidance_Modules_1-6-debug-extraction.txt\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pdfplumber\n",
    "import glob\n",
    "import hashlib\n",
    "from abc import ABC\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "url = \"https://analyzr-llama-33-70b-test.eastus2.models.ai.azure.com/chat/completions\"\n",
    "key = \"o5Ko0yHozfM8DYg9ogQe7lsx0SUXhJtL\"\n",
    "\n",
    "# Constants from your boilerplate\n",
    "REQUEST_TIMEOUT = 70  # in seconds\n",
    "LLM_TEMPERATURE = 0.5\n",
    "LLM_MAX_TOKENS = 3000\n",
    "LLM_ENDPOINTS = {\n",
    "    'llama3_3': {\n",
    "        'best': {\n",
    "            'url': url,  # URL GOES HERE\n",
    "            'key': key,  # KEY GOES HERE\n",
    "            'model_name': None,  # Only necessary if model requires it\n",
    "        },\n",
    "        'fast': {\n",
    "            'url': None,  # URL GOES HERE\n",
    "            'key': None,  # KEY GOES HERE\n",
    "            'model_name': None,  # Only necessary if model requires it\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Cache class for storing LLM responses\n",
    "class LLMResponseCache:\n",
    "    \"\"\"Cache for storing LLM responses to avoid repeated queries\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=\".cache\"):\n",
    "        \"\"\"Initialize the cache with a directory to store cache files\"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        self.cache = self._load_cache()\n",
    "    \n",
    "    def _generate_key(self, system, user, temperature, max_tokens):\n",
    "        \"\"\"Generate a unique key for the cache based on inputs\"\"\"\n",
    "        # Create a string with all parameters\n",
    "        key_string = f\"{system}|{user}|{temperature}|{max_tokens}\"\n",
    "        # Create a hash of this string for the key\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()\n",
    "    \n",
    "    def _get_cache_file_path(self):\n",
    "        \"\"\"Get the path to the cache file\"\"\"\n",
    "        return os.path.join(self.cache_dir, \"llm_response_cache.json\")\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load the cache from disk\"\"\"\n",
    "        cache_file = self._get_cache_file_path()\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cache: {str(e)}\")\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save the cache to disk\"\"\"\n",
    "        cache_file = self._get_cache_file_path()\n",
    "        try:\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cache: {str(e)}\")\n",
    "    \n",
    "    def get(self, system, user, temperature, max_tokens):\n",
    "        \"\"\"Get a response from the cache\"\"\"\n",
    "        key = self._generate_key(system, user, temperature, max_tokens)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def put(self, system, user, temperature, max_tokens, response):\n",
    "        \"\"\"Store a response in the cache\"\"\"\n",
    "        key = self._generate_key(system, user, temperature, max_tokens)\n",
    "        self.cache[key] = response\n",
    "        self._save_cache()\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the entire cache\"\"\"\n",
    "        self.cache = {}\n",
    "        self._save_cache()\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get statistics about the cache\"\"\"\n",
    "        return {\n",
    "            \"total_entries\": len(self.cache),\n",
    "            \"cache_size_bytes\": os.path.getsize(self._get_cache_file_path()) if os.path.exists(self._get_cache_file_path()) else 0\n",
    "        }\n",
    "\n",
    "# Keep your existing utility classes\n",
    "class g2mLLMClientBase(ABC):\n",
    "    \"\"\"Base class handling I/O with the LLM\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with cache\"\"\"\n",
    "        self.cache = LLMResponseCache()\n",
    "    \n",
    "    def setLLm(self, query_type='best'):\n",
    "        \"\"\"\n",
    "        Set the appropriate LLM, especially based on query_type attribute (e.g. 'best' || 'fast')\n",
    "\n",
    "        :param query_type:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._url = LLM_ENDPOINTS[self._type][query_type]['url'] \n",
    "        self._api_key = LLM_ENDPOINTS[self._type][query_type]['key']\n",
    "        self._model_name = LLM_ENDPOINTS[self._type][query_type].get('model_name', None)\n",
    "        self._type = self._type\n",
    "        self._query_type = query_type\n",
    "\n",
    "    def query(self, user='', system='', temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS, query_type='best', use_cache=True):\n",
    "        \"\"\"\n",
    "        Send query to LLM. The user query and system context are provided separately.\n",
    "        The full prompt is assembled here using the appropriate syntax. \n",
    "\n",
    "        :param user: user query, e.g. 'hello, how are you'\n",
    "        :param system: system context and role, e.g. 'you are business analyst'\n",
    "        :param temperature:\n",
    "        :param max_tokens:\n",
    "        :param query_type:\n",
    "        :param use_cache: whether to use the cache\n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        # Try to get from cache first if enabled\n",
    "        if use_cache:\n",
    "            cached_response = self.cache.get(system, user, temperature, max_tokens)\n",
    "            if cached_response:\n",
    "                print(\"Using cached LLM response\")\n",
    "                return cached_response\n",
    "        \n",
    "        # No cache hit, make a new request\n",
    "        self.setLLm(query_type=query_type)\n",
    "        body = {\n",
    "            'messages': [\n",
    "                {\n",
    "                    'role': 'system', \n",
    "                    'content': system, \n",
    "                }, \n",
    "                {\n",
    "                    'role': 'user', \n",
    "                    'content': user, \n",
    "                }, \n",
    "            ], \n",
    "            'temperature': temperature, \n",
    "            'max_tokens': max_tokens, \n",
    "        }\n",
    "        if self._model_name is not None:\n",
    "            print('Querying with model...', f'model_name={self._model_name}')\n",
    "            body['model'] = self._model_name\n",
    "        \n",
    "        response = self._send_request(body)\n",
    "        \n",
    "        # Cache the response if caching is enabled\n",
    "        if use_cache and response.get('status') == 'Successful':\n",
    "            self.cache.put(system, user, temperature, max_tokens, response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def _send_request(self, body):\n",
    "        \"\"\"\n",
    "        Send JSON request to LLM API\n",
    "\n",
    "        :param body: \n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self._url is not None and self._api_key is not None:\n",
    "                res = requests.post(\n",
    "                    self._url,\n",
    "                    json=body,\n",
    "                    headers={\n",
    "                        \"Accept\": \"*/*\",\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"Authorization\": \"Bearer {}\".format(self._api_key),\n",
    "                    },\n",
    "                    timeout=REQUEST_TIMEOUT, \n",
    "                )\n",
    "                obj = json.loads(res.content)\n",
    "                text = self._parse_response(obj)\n",
    "                \n",
    "                res = {'status': 'Successful', 'text': text}\n",
    "            elif self._url is None:\n",
    "                print('LLM URL not specified, aborting LLM query', f'url={self._url}')\n",
    "                res = {'status': 'Unavailable', 'message': 'URL not specified'}\n",
    "            else:\n",
    "                print('LLM access parameters not valid', f'url={self._url}')\n",
    "                res = {'status': 'Unavailable', 'message': 'Invalid access parameters'}\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Cannot send LLM request: {}'.format(e), f'url={self._url}')\n",
    "            res = {'status': 'Unavailable', 'message': f'Cannot send request: {str(e)}'}\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def _parse_response(self, obj):\n",
    "        \"\"\"\n",
    "        Parse LLM response\n",
    "\n",
    "        :param obj:\n",
    "        :return text:\n",
    "        \"\"\"\n",
    "        if 'object' in obj.keys() and obj['object'] == 'Error':\n",
    "            text = obj['message']\n",
    "        elif 'error' in obj.keys(): \n",
    "            # Check if 'message' is a stringified JSON\n",
    "            if isinstance(obj['error']['message'], str):\n",
    "                try:\n",
    "                    error_message = json.loads(obj['error']['message'])\n",
    "                    print(f'[_parse_response] LLM response returned with error: {error_message}')\n",
    "                    text = 'Unable to give a response.'\n",
    "                except json.JSONDecodeError: \n",
    "                    text = obj['error']['message']\n",
    "            else:\n",
    "                text = obj['error']['message'].get('message', 'Unable to give a response.')\n",
    "        else:\n",
    "            text = obj['choices'][0]['message']['content'].strip()\n",
    "        return text\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the response cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        print(\"Cache cleared\")\n",
    "\n",
    "class g2mLLMClientLlama(g2mLLMClientBase):\n",
    "    \"\"\"Class handling I/O with Llama LLM\"\"\"\n",
    "\n",
    "    def __init__(self, llm_type='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize class instance\"\"\"\n",
    "        super().__init__()  # Initialize the base class with cache\n",
    "        self._url = LLM_ENDPOINTS[llm_type][query_type]['url'] \n",
    "        self._api_key = LLM_ENDPOINTS[llm_type][query_type]['key']\n",
    "        self._type = llm_type\n",
    "        self._query_type = query_type\n",
    "\n",
    "class g2mPDFParser:\n",
    "    \"\"\"Class handling I/O with the LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, llm='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize class instance\"\"\"\n",
    "        match llm:\n",
    "            case 'llama3_1_small' | 'llama3_1_large' | 'llama3_3':\n",
    "                self.__llm = g2mLLMClientLlama(llm_type=llm, query_type=query_type)\n",
    "            case _:\n",
    "                print('Unknown LLM type', f'llm_type={llm}')\n",
    "                self.__llm = None \n",
    "        \n",
    "        # Default cache setting\n",
    "        self.use_cache = True\n",
    "\n",
    "    def read_in_text_file(self, file):\n",
    "        \"\"\"Read text from a file\"\"\"\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            string = f.read()\n",
    "        return string\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_paths(folder_path):\n",
    "        \"\"\"Get all file paths in a folder\"\"\"\n",
    "        file_paths = []\n",
    "        for root, directories, files in os.walk(folder_path):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                file_paths.append(file_path)\n",
    "        return file_paths\n",
    "        \n",
    "    def convert_to_text(self, pdf, filepath=None, save=True):\n",
    "        \"\"\"Convert PDF to text\"\"\"\n",
    "        with pdfplumber.open(pdf) as pdf2:\n",
    "            # Extract text from all pages, not just the first one\n",
    "            all_text = \"\"\n",
    "            for page in pdf2.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text += text + \"\\n\\n\"\n",
    "            \n",
    "            print(f\"Extracted {len(all_text)} characters from {pdf}\")\n",
    "            \n",
    "            # Save text to file\n",
    "            if save: \n",
    "                root, ext = os.path.splitext(pdf)\n",
    "                with open(f'{root}-pdfplumber.txt', \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(all_text)\n",
    "            \n",
    "        return all_text\n",
    "\n",
    "    def convert_pdfs(self, files, filepath=None):\n",
    "        \"\"\"Convert multiple PDFs to text\"\"\"\n",
    "        results = []\n",
    "        for file in files:\n",
    "            root, ext = os.path.splitext(file)\n",
    "            if ext.lower() == \".pdf\":\n",
    "                try: \n",
    "                    text = self.convert_to_text(file, filepath)\n",
    "                    results.append({\"file\": file, \"status\": \"success\", \"text\": text})\n",
    "                except Exception as e: \n",
    "                    print(f\"Warning! PDF could not be converted: {file}. Error: {str(e)}\")\n",
    "                    results.append({\"file\": file, \"status\": \"error\", \"message\": str(e)})\n",
    "        return results\n",
    "\n",
    "    def bulk_answer_and_save(self, system='', files=None, save=False, filepath=None, max_sections=None):\n",
    "        \"\"\"\n",
    "        Process multiple files with LLM\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process (for debugging)\n",
    "        \"\"\"\n",
    "        if files is None:\n",
    "            files = []\n",
    "            \n",
    "        results = []\n",
    "        for file in files: \n",
    "            # Handle the file extension\n",
    "            root, ext = os.path.splitext(file)\n",
    "            \n",
    "            try:\n",
    "                if ext.lower() == \".pdf\":\n",
    "                    # Convert PDF to text if needed\n",
    "                    text_file = f'{root}-pdfplumber.txt'\n",
    "                    if not os.path.exists(text_file):\n",
    "                        self.convert_to_text(file, filepath)\n",
    "                elif ext.lower() == \".txt\":\n",
    "                    # Already a text file\n",
    "                    text_file = file\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {ext}\")\n",
    "                    continue\n",
    "                \n",
    "                # Read the text content\n",
    "                user = self.read_in_text_file(text_file)\n",
    "                \n",
    "                # Process sections and query LLM\n",
    "                sections = self.split_text_into_sections(user)\n",
    "                all_questions = []\n",
    "                \n",
    "                # Apply section limit if specified\n",
    "                if max_sections is not None and max_sections > 0:\n",
    "                    print(f\"Debug mode: Processing only first {max_sections} of {len(sections)} sections\")\n",
    "                    sections = sections[:max_sections]\n",
    "                \n",
    "                for i, section in enumerate(sections):\n",
    "                    section_text, section_header = section\n",
    "                    print(f\"Processing section {i+1}/{len(sections)}: {section_header}\")\n",
    "                    \n",
    "                    # Query LLM for this section\n",
    "                    result = self.query(user=section_text, system=system, use_cache=self.use_cache)\n",
    "                    \n",
    "                    # Handle response\n",
    "                    if isinstance(result, dict) and 'text' in result:\n",
    "                        section_questions = result['text']\n",
    "                    else:\n",
    "                        try:\n",
    "                            obj = json.loads(result.content)\n",
    "                            section_questions = obj['choices'][0]['text']\n",
    "                        except:\n",
    "                            section_questions = f\"Error processing section {section_header}\"\n",
    "                    \n",
    "                    # Add to results\n",
    "                    all_questions.append({\n",
    "                        \"section\": section_header,\n",
    "                        \"questions\": section_questions\n",
    "                    })\n",
    "                    \n",
    "                # Save results if requested\n",
    "                if save:\n",
    "                    answer_file = f'{root}-questions.json'\n",
    "                    if filepath is not None:\n",
    "                        answer_file = os.path.join(filepath, os.path.basename(answer_file))\n",
    "                    \n",
    "                    with open(answer_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(all_questions, f, indent=2)\n",
    "                    \n",
    "                    print(f\"Saved questions to {answer_file}\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"file\": file,\n",
    "                    \"status\": \"success\",\n",
    "                    \"questions\": all_questions\n",
    "                })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "                results.append({\n",
    "                    \"file\": file,\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": str(e)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def query(self, user='', system='', temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS, query_type='best', use_cache=None):\n",
    "        \"\"\"\n",
    "        Query the LLM with optional cache control\n",
    "        \n",
    "        :param use_cache: Override instance cache setting\n",
    "        \"\"\"\n",
    "        # Determine whether to use cache\n",
    "        should_use_cache = self.use_cache if use_cache is None else use_cache\n",
    "        \n",
    "        if self.__llm is not None:\n",
    "            res = self.__llm.query(\n",
    "                user=user, \n",
    "                system=system, \n",
    "                temperature=temperature, \n",
    "                max_tokens=max_tokens, \n",
    "                query_type=query_type,\n",
    "                use_cache=should_use_cache\n",
    "            )\n",
    "        else:\n",
    "            print('LLM type unknown, aborting LLM query', f'type={self.__llm}')\n",
    "            res = {'status': 'Unavailable', 'message': 'LLM type unknown'}\n",
    "        return res\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the response cache\"\"\"\n",
    "        if self.__llm is not None:\n",
    "            self.__llm.clear_cache()\n",
    "    \n",
    "    def set_cache_enabled(self, enabled=True):\n",
    "        \"\"\"Set whether caching is enabled\"\"\"\n",
    "        self.use_cache = enabled\n",
    "        print(f\"Caching {'enabled' if enabled else 'disabled'}\")\n",
    "    \n",
    "    def split_text_into_sections(self, text):\n",
    "        \"\"\"\n",
    "        Split text into sections based on section headers like (1.2) or (1.2.3)\n",
    "        Returns a list of tuples: [(section_text, section_header), ...]\n",
    "        \"\"\"\n",
    "        # Pattern to match section headers like (1.2) or (1.2.3)\n",
    "        pattern = r'(\\(\\d+\\.\\d+(?:\\.\\d+)?\\))'\n",
    "        \n",
    "        # Find all matches\n",
    "        matches = list(re.finditer(pattern, text))\n",
    "        \n",
    "        sections = []\n",
    "        \n",
    "        # Process each match\n",
    "        for i in range(len(matches)):\n",
    "            # Get the current section header\n",
    "            header = matches[i].group(1)\n",
    "            \n",
    "            # Get the start of this section\n",
    "            start = matches[i].start()\n",
    "            \n",
    "            # Get the end of this section (start of next section or end of text)\n",
    "            if i < len(matches) - 1:\n",
    "                end = matches[i + 1].start()\n",
    "            else:\n",
    "                end = len(text)\n",
    "            \n",
    "            # Extract the section text\n",
    "            section_text = text[start:end].strip()\n",
    "            \n",
    "            # Add to our list of sections\n",
    "            sections.append((section_text, header))\n",
    "        \n",
    "        # If there's text before the first section, include it as a prologue\n",
    "        if matches and matches[0].start() > 0:\n",
    "            prologue = text[:matches[0].start()].strip()\n",
    "            if prologue:\n",
    "                sections.insert(0, (prologue, \"Prologue\"))\n",
    "        \n",
    "        # If no sections were found, return the entire text as one section\n",
    "        if not sections:\n",
    "            sections = [(text, \"Full Document\")]\n",
    "        \n",
    "        return sections\n",
    "\n",
    "class SectionProcessor:\n",
    "    \"\"\"Process text files by section and extract questions using LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_type='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize with LLM client\"\"\"\n",
    "        self.parser = g2mPDFParser(llm=llm_type, query_type=query_type)\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert at extracting questions from text content. \n",
    "        \n",
    "        Your task is to analyze the given section of text and identify any explicit or implicit questions it contains.\n",
    "        \n",
    "        For each question you identify:\n",
    "        1. Extract or formulate the complete question\n",
    "        2. Ensure the question makes sense on its own without needing additional context\n",
    "        3. Preserve the original meaning and intent of the question\n",
    "        \n",
    "        Return ONLY a numbered list of questions, with one question per line.\n",
    "        Do not include any explanations, summaries, or additional text.\n",
    "        \n",
    "        Important guidelines:\n",
    "        - Focus on extracting questions that are seeking specific information\n",
    "        - If the text contains incomplete questions, formulate them into complete questions\n",
    "        - Avoid creating questions that weren't implied in the original text\n",
    "        - Maintain the technical terminology and specificity of the original content\n",
    "        \"\"\"\n",
    "    \n",
    "    def process_file(self, file_path, output_dir=None, max_sections=None, use_cache=True):\n",
    "        \"\"\"\n",
    "        Process a single text file and extract questions by section\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process (for debugging)\n",
    "        :param use_cache: Whether to use the LLM response cache\n",
    "        \"\"\"\n",
    "        # Set cache configuration\n",
    "        self.parser.set_cache_enabled(use_cache)\n",
    "        \n",
    "        # Determine output directory\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.dirname(file_path)\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate output filename\n",
    "        base_name = os.path.basename(file_path)\n",
    "        name_without_ext = os.path.splitext(base_name)[0]\n",
    "        output_file = os.path.join(output_dir, f\"{name_without_ext}-questions.json\")\n",
    "        \n",
    "        # Read the file\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            # Convert PDF to text first\n",
    "            text = self.parser.convert_to_text(file_path, save=True)\n",
    "            if not text:\n",
    "                print(f\"Error: Could not extract text from PDF {file_path}\")\n",
    "                return None\n",
    "        else:\n",
    "            # Read text file directly\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "                return None\n",
    "        \n",
    "        # Split into sections\n",
    "        sections = self.parser.split_text_into_sections(text)\n",
    "        print(f\"Found {len(sections)} sections in {file_path}\")\n",
    "        \n",
    "        # Apply section limit if specified\n",
    "        if max_sections is not None and max_sections > 0:\n",
    "            print(f\"Debug mode: Processing only first {max_sections} of {len(sections)} sections\")\n",
    "            sections = sections[:max_sections]\n",
    "        \n",
    "        # Process each section\n",
    "        results = []\n",
    "        for i, (section_text, section_header) in enumerate(sections):\n",
    "            print(f\"Processing section {i+1}/{len(sections) if max_sections is None else max_sections}: {section_header}\")\n",
    "\n",
    "            \n",
    "            # Skip very short sections\n",
    "            if len(section_text.strip()) < 50:\n",
    "                print(f\"Skipping section {section_header} - too short\")\n",
    "                continue\n",
    "            \n",
    "            # Extract questions using LLM\n",
    "            try:\n",
    "                response = self.parser.query(\n",
    "                    user=section_text,\n",
    "                    system=self.system_prompt,\n",
    "                    temperature=0.3  # Lower temperature for more consistent outputs\n",
    "                )\n",
    "                \n",
    "                if isinstance(response, dict) and 'text' in response:\n",
    "                    # Process the questions - split by number and clean up\n",
    "                    questions_text = response['text']\n",
    "                    # Clean up the questions list\n",
    "                    questions = self.clean_questions_list(questions_text)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"section_id\": section_header,\n",
    "                        \"section_text\": section_text[:100] + \"...\" if len(section_text) > 100 else section_text,\n",
    "                        \"questions\": questions\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Error: Unexpected response format for section {section_header}\")\n",
    "                    results.append({\n",
    "                        \"section_id\": section_header,\n",
    "                        \"error\": \"Unexpected response format\",\n",
    "                        \"raw_response\": str(response)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing section {section_header}: {str(e)}\")\n",
    "                results.append({\n",
    "                    \"section_id\": section_header,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Save results to JSON file\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    \"file\": file_path,\n",
    "                    \"total_sections\": len(sections),\n",
    "                    \"processed_sections\": len(results),\n",
    "                    \"results\": results\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"Results saved to {output_file}\")\n",
    "            return output_file\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def process_directory(self, directory_path, output_dir=None, max_sections=None, use_cache=True):\n",
    "        \"\"\"\n",
    "        Process all text and PDF files in a directory\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process per file (for debugging)\n",
    "        :param use_cache: Whether to use the LLM response cache\n",
    "        \"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.join(directory_path, \"questions_output\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Get all text and PDF files\n",
    "        files = []\n",
    "        for extension in ['.txt', '.pdf']:\n",
    "            files.extend(glob.glob(os.path.join(directory_path, f\"*{extension}\")))\n",
    "        \n",
    "        results = []\n",
    "        for file in files:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            output_file = self.process_file(\n",
    "                file, \n",
    "                output_dir, \n",
    "                max_sections=max_sections,\n",
    "                use_cache=use_cache\n",
    "            )\n",
    "            if output_file:\n",
    "                results.append({\n",
    "                    \"input_file\": file,\n",
    "                    \"output_file\": output_file,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "            else:\n",
    "                results.append({\n",
    "                    \"input_file\": file,\n",
    "                    \"status\": \"error\"\n",
    "                })\n",
    "        \n",
    "        # Save summary\n",
    "        summary_file = os.path.join(output_dir, \"processing_summary.json\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"directory\": directory_path,\n",
    "                \"files_processed\": len(files),\n",
    "                \"successful\": sum(1 for r in results if r[\"status\"] == \"success\"),\n",
    "                \"failed\": sum(1 for r in results if r[\"status\"] == \"error\"),\n",
    "                \"details\": results\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        return summary_file\n",
    "    \n",
    "    def clean_questions_list(self, text):\n",
    "        \"\"\"Clean up the list of questions from LLM output\"\"\"\n",
    "        # Split by newlines first\n",
    "        lines = text.strip().split('\\n')\n",
    "        \n",
    "        # Clean list of questions\n",
    "        questions = []\n",
    "        for line in lines:\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            # Remove numbering and leading/trailing whitespace\n",
    "            # Match patterns like \"1.\", \"1)\", \"Question 1:\", etc.\n",
    "            cleaned = re.sub(r'^\\s*(\\d+[\\.\\):]|Question\\s+\\d+:?)\\s*', '', line).strip()\n",
    "            \n",
    "            if cleaned:\n",
    "                questions.append(cleaned)\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the LLM response cache\"\"\"\n",
    "        self.parser.clear_cache()\n",
    "        print(\"Cache cleared\")\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get statistics about the cache\"\"\"\n",
    "        if hasattr(self.parser, '_g2mPDFParser__llm') and self.parser._g2mPDFParser__llm is not None:\n",
    "            return self.parser._g2mPDFParser__llm.cache.get_cache_stats()\n",
    "        return {\"error\": \"LLM client not properly initialized\"}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the processor\n",
    "    processor = SectionProcessor(llm_type='llama3_3', query_type='best')\n",
    "    \n",
    "    # Create a sample text file for testing\n",
    "    input_file = \"pdf_files\\CDP_2024_Corporate_Questionnaire_Guidance_Modules_1-6-debug-extraction.txt\"\n",
    "    with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\"\"\n",
    "        Introduction to the document\n",
    "        \n",
    "        (1.1) First Section\n",
    "        This is content of the first section which discusses important concepts.\n",
    "        What are the key areas to focus on for this topic?\n",
    "        The document highlights several approaches to consider when implementing these ideas.\n",
    "        \n",
    "        (1.2) Second Section\n",
    "        In this section, we examine the relationship between various factors.\n",
    "        How do these factors influence outcomes?\n",
    "        It's important to understand the implications for practical applications.\n",
    "        \n",
    "        (2.1) Another Major Section\n",
    "        This section introduces new methodologies and frameworks.\n",
    "        Which framework is most appropriate for different scenarios?\n",
    "        Consider how these methodologies can be adapted to specific contexts.\n",
    "        \n",
    "        (2.1.1) Subsection with more detail\n",
    "        Here we dive deeper into specific aspects of the framework.\n",
    "        What are the limitations of this approach?\n",
    "        Several case studies demonstrate successful implementation.\n",
    "        \"\"\")\n",
    "    \n",
    "    # Process the sample file with caching enabled and limiting to 2 sections\n",
    "    print(\"Processing sample file (first 2 sections only)...\")\n",
    "    output_file = processor.process_file(input_file, max_sections=2, use_cache=True)\n",
    "    \n",
    "    # Print cache stats\n",
    "    print(\"Cache statistics:\", processor.get_cache_stats())\n",
    "    \n",
    "    # Process the same file again, should use cached responses for the first 2 sections\n",
    "    print(\"\\nProcessing sample file again (all sections)...\")\n",
    "    output_file = processor.process_file(input_file, use_cache=True)\n",
    "    \n",
    "    # Print updated cache stats\n",
    "    print(\"Cache statistics after second run:\", processor.get_cache_stats())\n",
    "    \n",
    "    # Clean up sample file\n",
    "    # os.remove(input_file)\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Results saved to {output_file}\")\n",
    "    \n",
    "    # Example of how to clear the cache if needed\n",
    "    # processor.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b396b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bbdab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
