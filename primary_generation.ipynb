{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1962c51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfplumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhashlib\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pdfplumber'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pdfplumber\n",
    "import glob\n",
    "import hashlib\n",
    "from abc import ABC\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "url = \"https://analyzr-llama-33-70b-test.eastus2.models.ai.azure.com/chat/completions\"\n",
    "key = \"o5Ko0yHozfM8DYg9ogQe7lsx0SUXhJtL\"\n",
    "deepseek_endpoint = 'https://DeepSeek-V3-jheyg.eastus2.models.ai.azure.com/v1/chat/completions'\n",
    "deepseek_key = 'onH6519Y4go0sJKQK0JoTOJ4Yf8oJcOF'\n",
    "\n",
    "# Constants from your boilerplate\n",
    "REQUEST_TIMEOUT = 70  # in seconds\n",
    "LLM_TEMPERATURE = 0.5\n",
    "LLM_MAX_TOKENS = 3000\n",
    "LLM_ENDPOINTS = {\n",
    "    'deepseek_v3': {\n",
    "        'best': {\n",
    "            'url': deepseek_endpoint,  # URL GOES HERE\n",
    "            'key': deepseek_key,  # KEY GOES HERE\n",
    "            'model_name': None,  # Only necessary if model requires it\n",
    "        },\n",
    "        'fast': {\n",
    "            'url': None,  # URL GOES HERE\n",
    "            'key': None,  # KEY GOES HERE\n",
    "            'model_name': None,  # Only necessary if model requires it\n",
    "        }\n",
    "    },\n",
    "    'llama3_3': {\n",
    "        'best': {\n",
    "            'url': url,  # URL GOES HERE\n",
    "            'key': key,  # KEY GOES HERE\n",
    "            'model_name': None,  # Only necessary if model requires it\n",
    "        },\n",
    "        'fast': {\n",
    "            'url': None,  # URL GOES HERE\n",
    "            'key': None,  # KEY GOES HERE\n",
    "            'model_name': None,  # Only necessary if model requires it\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Cache class for storing LLM responses\n",
    "class LLMResponseCache:\n",
    "    \"\"\"Cache for storing LLM responses to avoid repeated queries\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=\".cache\"):\n",
    "        \"\"\"Initialize the cache with a directory to store cache files\"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        self.cache = self._load_cache()\n",
    "    \n",
    "    def _generate_key(self, system, user, temperature, max_tokens):\n",
    "        \"\"\"Generate a unique key for the cache based on inputs\"\"\"\n",
    "        # Create a string with all parameters\n",
    "        key_string = f\"{system}|{user}|{temperature}|{max_tokens}\"\n",
    "        # Create a hash of this string for the key\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()\n",
    "    \n",
    "    def _get_cache_file_path(self):\n",
    "        \"\"\"Get the path to the cache file\"\"\"\n",
    "        return os.path.join(self.cache_dir, \"llm_response_cache.json\")\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load the cache from disk\"\"\"\n",
    "        cache_file = self._get_cache_file_path()\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cache: {str(e)}\")\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save the cache to disk\"\"\"\n",
    "        cache_file = self._get_cache_file_path()\n",
    "        try:\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.cache, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cache: {str(e)}\")\n",
    "    \n",
    "    def get(self, system, user, temperature, max_tokens):\n",
    "        \"\"\"Get a response from the cache\"\"\"\n",
    "        key = self._generate_key(system, user, temperature, max_tokens)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def put(self, system, user, temperature, max_tokens, response):\n",
    "        \"\"\"Store a response in the cache\"\"\"\n",
    "        key = self._generate_key(system, user, temperature, max_tokens)\n",
    "        self.cache[key] = response\n",
    "        self._save_cache()\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the entire cache\"\"\"\n",
    "        self.cache = {}\n",
    "        self._save_cache()\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get statistics about the cache\"\"\"\n",
    "        return {\n",
    "            \"total_entries\": len(self.cache),\n",
    "            \"cache_size_bytes\": os.path.getsize(self._get_cache_file_path()) if os.path.exists(self._get_cache_file_path()) else 0\n",
    "        }\n",
    "\n",
    "# Keep your existing utility classes\n",
    "class g2mLLMClientBase(ABC):\n",
    "    \"\"\"Base class handling I/O with the LLM\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with cache\"\"\"\n",
    "        self.cache = LLMResponseCache()\n",
    "    \n",
    "    def setLLm(self, query_type='best'):\n",
    "        \"\"\"\n",
    "        Set the appropriate LLM, especially based on query_type attribute (e.g. 'best' || 'fast')\n",
    "\n",
    "        :param query_type:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._url = LLM_ENDPOINTS[self._type][query_type]['url'] \n",
    "        self._api_key = LLM_ENDPOINTS[self._type][query_type]['key']\n",
    "        self._model_name = LLM_ENDPOINTS[self._type][query_type].get('model_name', None)\n",
    "        self._type = self._type\n",
    "        self._query_type = query_type\n",
    "\n",
    "    def query(self, user='', system='', temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS, query_type='best', use_cache=True):\n",
    "        \"\"\"\n",
    "        Send query to LLM. The user query and system context are provided separately.\n",
    "        The full prompt is assembled here using the appropriate syntax. \n",
    "\n",
    "        :param user: user query, e.g. 'hello, how are you'\n",
    "        :param system: system context and role, e.g. 'you are business analyst'\n",
    "        :param temperature:\n",
    "        :param max_tokens:\n",
    "        :param query_type:\n",
    "        :param use_cache: whether to use the cache\n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        # Try to get from cache first if enabled\n",
    "        if use_cache:\n",
    "            cached_response = self.cache.get(system, user, temperature, max_tokens)\n",
    "            if cached_response:\n",
    "                print(\"Using cached LLM response\")\n",
    "                return cached_response\n",
    "        \n",
    "        # No cache hit, make a new request\n",
    "        self.setLLm(query_type=query_type)\n",
    "        body = {\n",
    "            'messages': [\n",
    "                {\n",
    "                    'role': 'system', \n",
    "                    'content': system, \n",
    "                }, \n",
    "                {\n",
    "                    'role': 'user', \n",
    "                    'content': user, \n",
    "                }, \n",
    "            ], \n",
    "            'temperature': temperature, \n",
    "            'max_tokens': max_tokens, \n",
    "        }\n",
    "        if self._model_name is not None:\n",
    "            print('Querying with model...', f'model_name={self._model_name}')\n",
    "            body['model'] = self._model_name\n",
    "        \n",
    "        response = self._send_request(body)\n",
    "        \n",
    "        # Cache the response if caching is enabled\n",
    "        if use_cache and response.get('status') == 'Successful':\n",
    "            self.cache.put(system, user, temperature, max_tokens, response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def _send_request(self, body):\n",
    "        \"\"\"\n",
    "        Send JSON request to LLM API\n",
    "\n",
    "        :param body: \n",
    "        :return res:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self._url is not None and self._api_key is not None:\n",
    "                res = requests.post(\n",
    "                    self._url,\n",
    "                    json=body,\n",
    "                    headers={\n",
    "                        \"Accept\": \"*/*\",\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"Authorization\": \"Bearer {}\".format(self._api_key),\n",
    "                    },\n",
    "                    timeout=REQUEST_TIMEOUT, \n",
    "                )\n",
    "                obj = json.loads(res.content)\n",
    "                text = self._parse_response(obj)\n",
    "                \n",
    "                res = {'status': 'Successful', 'text': text}\n",
    "            elif self._url is None:\n",
    "                print('LLM URL not specified, aborting LLM query', f'url={self._url}')\n",
    "                res = {'status': 'Unavailable', 'message': 'URL not specified'}\n",
    "            else:\n",
    "                print('LLM access parameters not valid', f'url={self._url}')\n",
    "                res = {'status': 'Unavailable', 'message': 'Invalid access parameters'}\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Cannot send LLM request: {}'.format(e), f'url={self._url}')\n",
    "            res = {'status': 'Unavailable', 'message': f'Cannot send request: {str(e)}'}\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def _parse_response(self, obj):\n",
    "        \"\"\"\n",
    "        Parse LLM response\n",
    "\n",
    "        :param obj:\n",
    "        :return text:\n",
    "        \"\"\"\n",
    "        if 'object' in obj.keys() and obj['object'] == 'Error':\n",
    "            text = obj['message']\n",
    "        elif 'error' in obj.keys(): \n",
    "            # Check if 'message' is a stringified JSON\n",
    "            if isinstance(obj['error']['message'], str):\n",
    "                try:\n",
    "                    error_message = json.loads(obj['error']['message'])\n",
    "                    print(f'[_parse_response] LLM response returned with error: {error_message}')\n",
    "                    text = 'Unable to give a response.'\n",
    "                except json.JSONDecodeError: \n",
    "                    text = obj['error']['message']\n",
    "            else:\n",
    "                text = obj['error']['message'].get('message', 'Unable to give a response.')\n",
    "        else:\n",
    "            text = obj['choices'][0]['message']['content'].strip()\n",
    "        return text\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the response cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        print(\"Cache cleared\")\n",
    "\n",
    "class g2mLLMClientLlama(g2mLLMClientBase):\n",
    "    \"\"\"Class handling I/O with Llama LLM\"\"\"\n",
    "\n",
    "    def __init__(self, llm_type='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize class instance\"\"\"\n",
    "        super().__init__()  # Initialize the base class with cache\n",
    "        self._url = LLM_ENDPOINTS[llm_type][query_type]['url'] \n",
    "        self._api_key = LLM_ENDPOINTS[llm_type][query_type]['key']\n",
    "        self._type = llm_type\n",
    "        self._query_type = query_type\n",
    "\n",
    "class g2mPDFParser:\n",
    "    \"\"\"Class handling I/O with the LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, llm='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize class instance\"\"\"\n",
    "        match llm:\n",
    "            case 'llama3_1_small' | 'llama3_1_large' | 'llama3_3':\n",
    "                self.__llm = g2mLLMClientLlama(llm_type=llm, query_type=query_type)\n",
    "            case _:\n",
    "                print('Unknown LLM type', f'llm_type={llm}')\n",
    "                self.__llm = None \n",
    "        \n",
    "        # Default cache setting\n",
    "        self.use_cache = True\n",
    "\n",
    "    def read_in_text_file(self, file):\n",
    "        \"\"\"Read text from a file\"\"\"\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            string = f.read()\n",
    "        return string\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_paths(folder_path):\n",
    "        \"\"\"Get all file paths in a folder\"\"\"\n",
    "        file_paths = []\n",
    "        for root, directories, files in os.walk(folder_path):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                file_paths.append(file_path)\n",
    "        return file_paths\n",
    "        \n",
    "    def convert_to_text(self, pdf, filepath=None, save=True):\n",
    "        \"\"\"Convert PDF to text\"\"\"\n",
    "        with pdfplumber.open(pdf) as pdf2:\n",
    "            # Extract text from all pages, not just the first one\n",
    "            all_text = \"\"\n",
    "            for page in pdf2.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    all_text += text + \"\\n\\n\"\n",
    "            \n",
    "            print(f\"Extracted {len(all_text)} characters from {pdf}\")\n",
    "            \n",
    "            # Save text to file\n",
    "            if save: \n",
    "                root, ext = os.path.splitext(pdf)\n",
    "                with open(f'{root}-pdfplumber.txt', \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(all_text)\n",
    "            \n",
    "        return all_text\n",
    "\n",
    "    def convert_pdfs(self, files, filepath=None):\n",
    "        \"\"\"Convert multiple PDFs to text\"\"\"\n",
    "        results = []\n",
    "        for file in files:\n",
    "            root, ext = os.path.splitext(file)\n",
    "            if ext.lower() == \".pdf\":\n",
    "                try: \n",
    "                    text = self.convert_to_text(file, filepath)\n",
    "                    results.append({\"file\": file, \"status\": \"success\", \"text\": text})\n",
    "                except Exception as e: \n",
    "                    print(f\"Warning! PDF could not be converted: {file}. Error: {str(e)}\")\n",
    "                    results.append({\"file\": file, \"status\": \"error\", \"message\": str(e)})\n",
    "        return results\n",
    "\n",
    "    def bulk_answer_and_save(self, system='', files=None, save=False, filepath=None, max_sections=None):\n",
    "        \"\"\"\n",
    "        Process multiple files with LLM\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process (for debugging)\n",
    "        \"\"\"\n",
    "        if files is None:\n",
    "            files = []\n",
    "            \n",
    "        results = []\n",
    "        for file in files: \n",
    "            # Handle the file extension\n",
    "            root, ext = os.path.splitext(file)\n",
    "            \n",
    "            try:\n",
    "                if ext.lower() == \".pdf\":\n",
    "                    # Convert PDF to text if needed\n",
    "                    text_file = f'{root}-pdfplumber.txt'\n",
    "                    if not os.path.exists(text_file):\n",
    "                        self.convert_to_text(file, filepath)\n",
    "                elif ext.lower() == \".txt\":\n",
    "                    # Already a text file\n",
    "                    text_file = file\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {ext}\")\n",
    "                    continue\n",
    "                \n",
    "                # Read the text content\n",
    "                user = self.read_in_text_file(text_file)\n",
    "                \n",
    "                # Process sections and query LLM\n",
    "                sections = self.split_text_into_sections(user)\n",
    "                all_questions = []\n",
    "                \n",
    "                # Apply section limit if specified\n",
    "                if max_sections is not None and max_sections > 0:\n",
    "                    print(f\"Debug mode: Processing only first {max_sections} of {len(sections)} sections\")\n",
    "                    sections = sections[:max_sections]\n",
    "                \n",
    "                for i, section in enumerate(sections):\n",
    "                    section_text, section_header = section\n",
    "                    print(f\"Processing section {i+1}/{len(sections)}: {section_header}\")\n",
    "                    \n",
    "                    # Query LLM for this section\n",
    "                    result = self.query(user=section_text, system=system, use_cache=self.use_cache)\n",
    "                    \n",
    "                    # Handle response\n",
    "                    if isinstance(result, dict) and 'text' in result:\n",
    "                        section_questions = result['text']\n",
    "                    else:\n",
    "                        try:\n",
    "                            obj = json.loads(result.content)\n",
    "                            section_questions = obj['choices'][0]['text']\n",
    "                        except:\n",
    "                            section_questions = f\"Error processing section {section_header}\"\n",
    "                    \n",
    "                    # Add to results\n",
    "                    all_questions.append({\n",
    "                        \"section\": section_header,\n",
    "                        \"questions\": section_questions\n",
    "                    })\n",
    "                    \n",
    "                # Save results if requested\n",
    "                if save:\n",
    "                    answer_file = f'{root}-questions.json'\n",
    "                    if filepath is not None:\n",
    "                        answer_file = os.path.join(filepath, os.path.basename(answer_file))\n",
    "                    \n",
    "                    with open(answer_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(all_questions, f, indent=2)\n",
    "                    \n",
    "                    print(f\"Saved questions to {answer_file}\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"file\": file,\n",
    "                    \"status\": \"success\",\n",
    "                    \"questions\": all_questions\n",
    "                })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "                results.append({\n",
    "                    \"file\": file,\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": str(e)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def query(self, user='', system='', temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS, query_type='best', use_cache=None):\n",
    "        \"\"\"\n",
    "        Query the LLM with optional cache control\n",
    "        \n",
    "        :param use_cache: Override instance cache setting\n",
    "        \"\"\"\n",
    "        # Determine whether to use cache\n",
    "        should_use_cache = self.use_cache if use_cache is None else use_cache\n",
    "        \n",
    "        if self.__llm is not None:\n",
    "            res = self.__llm.query(\n",
    "                user=user, \n",
    "                system=system, \n",
    "                temperature=temperature, \n",
    "                max_tokens=max_tokens, \n",
    "                query_type=query_type,\n",
    "                use_cache=should_use_cache\n",
    "            )\n",
    "        else:\n",
    "            print('LLM type unknown, aborting LLM query', f'type={self.__llm}')\n",
    "            res = {'status': 'Unavailable', 'message': 'LLM type unknown'}\n",
    "        return res\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the response cache\"\"\"\n",
    "        if self.__llm is not None:\n",
    "            self.__llm.clear_cache()\n",
    "    \n",
    "    def set_cache_enabled(self, enabled=True):\n",
    "        \"\"\"Set whether caching is enabled\"\"\"\n",
    "        self.use_cache = enabled\n",
    "        print(f\"Caching {'enabled' if enabled else 'disabled'}\")\n",
    "    \n",
    "    def split_text_into_sections(self, text):\n",
    "        \"\"\"\n",
    "        Split text into sections based on section headers like (1.2) or (1.2.3)\n",
    "        Returns a list of tuples: [(section_text, section_header), ...]\n",
    "        \"\"\"\n",
    "        # Pattern to match section headers like (1.2) or (1.2.3)\n",
    "        pattern = r'(\\(\\d+\\.\\d+(?:\\.\\d+)?\\))'\n",
    "        \n",
    "        # Find all matches\n",
    "        matches = list(re.finditer(pattern, text))\n",
    "        \n",
    "        sections = []\n",
    "        \n",
    "        # Process each match\n",
    "        for i in range(len(matches)):\n",
    "            # Get the current section header\n",
    "            header = matches[i].group(1)\n",
    "            \n",
    "            # Get the start of this section\n",
    "            start = matches[i].start()\n",
    "            \n",
    "            # Get the end of this section (start of next section or end of text)\n",
    "            if i < len(matches) - 1:\n",
    "                end = matches[i + 1].start()\n",
    "            else:\n",
    "                end = len(text)\n",
    "            \n",
    "            # Extract the section text\n",
    "            section_text = text[start:end].strip()\n",
    "            \n",
    "            # Add to our list of sections\n",
    "            sections.append((section_text, header))\n",
    "        \n",
    "        # If there's text before the first section, include it as a prologue\n",
    "        if matches and matches[0].start() > 0:\n",
    "            prologue = text[:matches[0].start()].strip()\n",
    "            if prologue:\n",
    "                sections.insert(0, (prologue, \"Prologue\"))\n",
    "        \n",
    "        # If no sections were found, return the entire text as one section\n",
    "        if not sections:\n",
    "            sections = [(text, \"Full Document\")]\n",
    "        \n",
    "        return sections\n",
    "\n",
    "class SectionProcessor:\n",
    "    \"\"\"Process text files by section and extract questions and answers using LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_type='llama3_3', query_type='best'):\n",
    "        \"\"\"Initialize with LLM client\"\"\"\n",
    "        self.parser = g2mPDFParser(llm=llm_type, query_type=query_type)\n",
    "        # Updated system prompt to extract both questions and answers\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert at extracting questions and their answers from text content.\n",
    "        \n",
    "        Your task is to analyze the given section of text and identify any explicit or implicit questions it contains,\n",
    "        along with their corresponding answers from the text.\n",
    "        \n",
    "        For each question you identify:\n",
    "        1. Extract or formulate the complete question\n",
    "        2. Extract or formulate the complete answer based on the text\n",
    "        3. Ensure both question and answer make sense on their own without needing additional context\n",
    "        4. Preserve the original meaning and intent\n",
    "        \n",
    "        Return your response in JSON format as follows:\n",
    "        ```json\n",
    "        {\n",
    "          \"qa_pairs\": [\n",
    "            {\n",
    "              \"question\": \"The full question text\",\n",
    "              \"answer\": \"The full answer text from the document\"\n",
    "            },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "        ```\n",
    "        \n",
    "        Important guidelines:\n",
    "        - Focus on extracting questions that are seeking specific information\n",
    "        - If the text contains incomplete questions, formulate them into complete questions\n",
    "        - If answers are implied rather than explicitly stated, formulate them based on the text\n",
    "        - Avoid creating questions or answers that weren't implied in the original text\n",
    "        - Maintain the technical terminology and specificity of the original content\n",
    "        - Include ALL text that constitutes the answer - don't truncate\n",
    "        \"\"\"\n",
    "    \n",
    "    def process_file(self, file_path, output_dir=None, max_sections=None, use_cache=True):\n",
    "        \"\"\"\n",
    "        Process a single text file and extract questions and answers by section\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process (for debugging)\n",
    "        :param use_cache: Whether to use the LLM response cache\n",
    "        \"\"\"\n",
    "        # Set cache configuration\n",
    "        self.parser.set_cache_enabled(use_cache)\n",
    "        \n",
    "        # Determine output directory\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.dirname(file_path)\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate output filename\n",
    "        base_name = os.path.basename(file_path)\n",
    "        name_without_ext = os.path.splitext(base_name)[0]\n",
    "        output_file = os.path.join(output_dir, f\"{name_without_ext}-qa-pairs.json\")\n",
    "        \n",
    "        # Read the file\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            # Convert PDF to text first\n",
    "            text = self.parser.convert_to_text(file_path, save=True)\n",
    "            if not text:\n",
    "                print(f\"Error: Could not extract text from PDF {file_path}\")\n",
    "                return None\n",
    "        else:\n",
    "            # Read text file directly\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "                return None\n",
    "        \n",
    "        # Split into sections\n",
    "        sections = self.parser.split_text_into_sections(text)\n",
    "        print(f\"Found {len(sections)} sections in {file_path}\")\n",
    "        \n",
    "        # Apply section limit if specified\n",
    "        if max_sections is not None and max_sections > 0:\n",
    "            print(f\"Debug mode: Processing only first {max_sections} of {len(sections)} sections\")\n",
    "            sections = sections[:max_sections]\n",
    "        \n",
    "        # Process each section\n",
    "        results = []\n",
    "        for i, (section_text, section_header) in enumerate(sections):\n",
    "            print(f\"Processing section {i+1}/{len(sections) if max_sections is None else max_sections}: {section_header}\")\n",
    "            \n",
    "            # Skip very short sections\n",
    "            if len(section_text.strip()) < 50:\n",
    "                print(f\"Skipping section {section_header} - too short\")\n",
    "                continue\n",
    "            \n",
    "            # Extract questions and answers using LLM\n",
    "            try:\n",
    "                response = self.parser.query(\n",
    "                    user=section_text,\n",
    "                    system=self.system_prompt,\n",
    "                    temperature=0.3,  # Lower temperature for more consistent outputs\n",
    "                    max_tokens=4000   # Increased max tokens to ensure full answers\n",
    "                )\n",
    "                \n",
    "                if isinstance(response, dict) and 'text' in response:\n",
    "                    # Process the response - extract JSON\n",
    "                    qa_text = response['text']\n",
    "                    \n",
    "                    # Extract JSON from the response\n",
    "                    try:\n",
    "                        # Look for JSON content between ```json and ``` if present\n",
    "                        json_match = re.search(r'```json\\s*(.*?)\\s*```', qa_text, re.DOTALL)\n",
    "                        if json_match:\n",
    "                            qa_json = json.loads(json_match.group(1))\n",
    "                        else:\n",
    "                            # Try parsing the entire response as JSON\n",
    "                            qa_json = json.loads(qa_text)\n",
    "                        \n",
    "                        # Add to results\n",
    "                        results.append({\n",
    "                            \"section_id\": section_header,\n",
    "                            \"section_text\": section_text,  # Store the full section text\n",
    "                            \"qa_pairs\": qa_json.get(\"qa_pairs\", [])\n",
    "                        })\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON response for section {section_header}: {str(e)}\")\n",
    "                        print(f\"Raw response: {qa_text[:100]}...\")\n",
    "                        results.append({\n",
    "                            \"section_id\": section_header,\n",
    "                            \"error\": \"JSON parsing error\",\n",
    "                            \"raw_response\": qa_text\n",
    "                        })\n",
    "                else:\n",
    "                    print(f\"Error: Unexpected response format for section {section_header}\")\n",
    "                    results.append({\n",
    "                        \"section_id\": section_header,\n",
    "                        \"error\": \"Unexpected response format\",\n",
    "                        \"raw_response\": str(response)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing section {section_header}: {str(e)}\")\n",
    "                results.append({\n",
    "                    \"section_id\": section_header,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Save results to JSON file\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    \"file\": file_path,\n",
    "                    \"total_sections\": len(sections),\n",
    "                    \"processed_sections\": len(results),\n",
    "                    \"results\": results\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"Results saved to {output_file}\")\n",
    "            return output_file\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def process_directory(self, directory_path, output_dir=None, max_sections=None, use_cache=True):\n",
    "        \"\"\"\n",
    "        Process all text and PDF files in a directory\n",
    "        \n",
    "        :param max_sections: Maximum number of sections to process per file (for debugging)\n",
    "        :param use_cache: Whether to use the LLM response cache\n",
    "        \"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.join(directory_path, \"questions_output\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Get all text and PDF files\n",
    "        files = []\n",
    "        for extension in ['.txt', '.pdf']:\n",
    "            files.extend(glob.glob(os.path.join(directory_path, f\"*{extension}\")))\n",
    "        \n",
    "        results = []\n",
    "        for file in files:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            output_file = self.process_file(\n",
    "                file, \n",
    "                output_dir, \n",
    "                max_sections=max_sections,\n",
    "                use_cache=use_cache\n",
    "            )\n",
    "            if output_file:\n",
    "                results.append({\n",
    "                    \"input_file\": file,\n",
    "                    \"output_file\": output_file,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "            else:\n",
    "                results.append({\n",
    "                    \"input_file\": file,\n",
    "                    \"status\": \"error\"\n",
    "                })\n",
    "        \n",
    "        # Save summary\n",
    "        summary_file = os.path.join(output_dir, \"processing_summary.json\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"directory\": directory_path,\n",
    "                \"files_processed\": len(files),\n",
    "                \"successful\": sum(1 for r in results if r[\"status\"] == \"success\"),\n",
    "                \"failed\": sum(1 for r in results if r[\"status\"] == \"error\"),\n",
    "                \"details\": results\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        return summary_file\n",
    "    \n",
    "    def clean_questions_list(self, text):\n",
    "        \"\"\"Clean up the list of questions from LLM output\"\"\"\n",
    "        # Split by newlines first\n",
    "        lines = text.strip().split('\\n')\n",
    "        \n",
    "        # Clean list of questions\n",
    "        questions = []\n",
    "        for line in lines:\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            # Remove numbering and leading/trailing whitespace\n",
    "            # Match patterns like \"1.\", \"1)\", \"Question 1:\", etc.\n",
    "            cleaned = re.sub(r'^\\s*(\\d+[\\.\\):]|Question\\s+\\d+:?)\\s*', '', line).strip()\n",
    "            \n",
    "            if cleaned:\n",
    "                questions.append(cleaned)\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the LLM response cache\"\"\"\n",
    "        self.parser.clear_cache()\n",
    "        print(\"Cache cleared\")\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get statistics about the cache\"\"\"\n",
    "        if hasattr(self.parser, '_g2mPDFParser__llm') and self.parser._g2mPDFParser__llm is not None:\n",
    "            return self.parser._g2mPDFParser__llm.cache.get_cache_stats()\n",
    "        return {\"error\": \"LLM client not properly initialized\"}\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize the processor\n",
    "#     processor = SectionProcessor(llm_type='llama3_3', query_type='best')\n",
    "    \n",
    "#     # Create a sample text file for testing\n",
    "#     input_file = r\"pdf_files\\Full_Corporate_Questionnaire_Modules_8-13.pdf\"\n",
    "#     # with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     #     f.write(\"\"\"\n",
    "#     #     Introduction to the document\n",
    "        \n",
    "#     #     (1.1) First Section\n",
    "#     #     This is content of the first section which discusses important concepts.\n",
    "#     #     What are the key areas to focus on for this topic?\n",
    "#     #     The document highlights several approaches to consider when implementing these ideas.\n",
    "        \n",
    "#     #     (1.2) Second Section\n",
    "#     #     In this section, we examine the relationship between various factors.\n",
    "#     #     How do these factors influence outcomes?\n",
    "#     #     It's important to understand the implications for practical applications.\n",
    "        \n",
    "#     #     (2.1) Another Major Section\n",
    "#     #     This section introduces new methodologies and frameworks.\n",
    "#     #     Which framework is most appropriate for different scenarios?\n",
    "#     #     Consider how these methodologies can be adapted to specific contexts.\n",
    "        \n",
    "#     #     (2.1.1) Subsection with more detail\n",
    "#     #     Here we dive deeper into specific aspects of the framework.\n",
    "#     #     What are the limitations of this approach?\n",
    "#     #     Several case studies demonstrate successful implementation.\n",
    "#     #     \"\"\")\n",
    "    \n",
    "#     # Process the sample file with caching enabled and limiting to 2 sections\n",
    "#     print(\"Processing sample file (first 2 sections only)...\")\n",
    "#     output_file = processor.process_file(input_file, max_sections=2, use_cache=True)\n",
    "    \n",
    "#     # Print cache stats\n",
    "#     print(\"Cache statistics:\", processor.get_cache_stats())\n",
    "    \n",
    "#     # Process the same file again, should use cached responses for the first 2 sections\n",
    "#     print(\"\\nProcessing sample file again (all sections)...\")\n",
    "#     output_file = processor.process_file(input_file, use_cache=True)\n",
    "    \n",
    "#     # Print updated cache stats\n",
    "#     print(\"Cache statistics after second run:\", processor.get_cache_stats())\n",
    "    \n",
    "#     # Clean up sample file\n",
    "#     # os.remove(input_file)\n",
    "    \n",
    "#     print(f\"\\nProcessing complete. Results saved to {output_file}\")\n",
    "    \n",
    "#     # Example of how to clear the cache if needed\n",
    "#     # processor.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def convert_qa_json_to_csv(json_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Convert the Q&A JSON output to CSV format\n",
    "    \n",
    "    Parameters:\n",
    "    - json_file_path: Path to the input JSON file\n",
    "    - output_csv_path: Path for the output CSV file (default: same name with .csv extension)\n",
    "    \"\"\"\n",
    "    # Determine output path if not specified\n",
    "    if not output_csv_path:\n",
    "        base_name = os.path.splitext(json_file_path)[0]\n",
    "        output_csv_path = f\"{base_name}.csv\"\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Prepare for CSV writing\n",
    "    with open(output_csv_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=['file_name', 'section_id', 'question', 'answer'])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Get the file name from the full path\n",
    "        file_name = os.path.basename(data.get('file', 'unknown'))\n",
    "        \n",
    "        # Process each section in the results\n",
    "        for section in data.get('results', []):\n",
    "            section_id = section.get('section_id', 'unknown')\n",
    "            \n",
    "            # Skip sections with errors\n",
    "            if 'error' in section:\n",
    "                continue\n",
    "            \n",
    "            # Process each Q&A pair\n",
    "            for qa_pair in section.get('qa_pairs', []):\n",
    "                question = qa_pair.get('question', '').replace('\\n', ' ').strip()\n",
    "                answer = qa_pair.get('answer', '').replace('\\n', ' ').strip()\n",
    "                \n",
    "                # Write the row\n",
    "                writer.writerow({\n",
    "                    'file_name': file_name,\n",
    "                    'section_id': section_id,\n",
    "                    'question': question,\n",
    "                    'answer': answer\n",
    "                })\n",
    "    \n",
    "    print(f\"Successfully converted {json_file_path} to {output_csv_path}\")\n",
    "    return output_csv_path\n",
    "\n",
    "# # Run the conversion\n",
    "# json_file = 'json_outputs\\\\Jake_First_Output-qa-pairs.json'\n",
    "# output_csv = 'json_outputs\\\\Jake_First_Output-qa-pairs.csv'\n",
    "# convert_qa_json_to_csv(json_file, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0696c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paste'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa_output\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your desired output folder\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Process all PDFs and generate CSVs\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m results \u001b[38;5;241m=\u001b[39m process_pdfs_to_csv(\n\u001b[0;32m    128\u001b[0m     pdf_directory, \n\u001b[0;32m    129\u001b[0m     output_directory,\n\u001b[0;32m    130\u001b[0m     max_sections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Set to a number for debugging\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m      \u001b[38;5;66;03m# Use caching to avoid repeated LLM calls\u001b[39;00m\n\u001b[0;32m    132\u001b[0m )\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mprocess_pdfs_to_csv\u001b[1;34m(pdf_directory, output_directory, max_sections, use_cache)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mProcess all PDF files in a directory, extract Q&A pairs, and save as both JSON and CSV\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m- use_cache: Whether to use the LLM response cache\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Import the processor class\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaste\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SectionProcessor\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Set output directory if not specified\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_directory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'paste'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def process_pdfs_to_csv(pdf_directory, output_directory=None, max_sections=None, use_cache=True):\n",
    "    \"\"\"\n",
    "    Process all PDF files in a directory, extract Q&A pairs, and save as both JSON and CSV\n",
    "    \n",
    "    Parameters:\n",
    "    - pdf_directory: Path to the directory containing PDF files\n",
    "    - output_directory: Path for output files (default: subdirectory of pdf_directory)\n",
    "    - max_sections: Maximum number of sections to process per file (for debugging)\n",
    "    - use_cache: Whether to use the LLM response cache\n",
    "    \"\"\"\n",
    "    # Import the processor class\n",
    "    from paste import SectionProcessor\n",
    "    \n",
    "    # Set output directory if not specified\n",
    "    if output_directory is None:\n",
    "        output_directory = os.path.join(pdf_directory, \"output\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = SectionProcessor(llm_type='llama3_3', query_type='best')\n",
    "    processor.set_cache_enabled(use_cache)\n",
    "    \n",
    "    # Get all PDF files in the directory\n",
    "    pdf_files = glob.glob(os.path.join(pdf_directory, \"*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    # Process each PDF file\n",
    "    json_files = []\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing PDF: {os.path.basename(pdf_file)}\")\n",
    "        \n",
    "        try:\n",
    "            # Process the PDF and generate JSON\n",
    "            json_file = processor.process_file(\n",
    "                pdf_file, \n",
    "                output_directory, \n",
    "                max_sections=max_sections,\n",
    "                use_cache=use_cache\n",
    "            )\n",
    "            \n",
    "            if json_file:\n",
    "                json_files.append(json_file)\n",
    "                print(f\"Successfully processed {pdf_file} to {json_file}\")\n",
    "            else:\n",
    "                print(f\"Failed to process {pdf_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {str(e)}\")\n",
    "    \n",
    "    # Convert all generated JSON files to CSV\n",
    "    csv_files = []\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            # Determine CSV output path\n",
    "            base_name = os.path.splitext(json_file)[0]\n",
    "            csv_file = f\"{base_name}.csv\"\n",
    "            \n",
    "            print(f\"Converting JSON to CSV: {os.path.basename(json_file)}\")\n",
    "            \n",
    "            # Load the JSON data\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Write CSV file\n",
    "            with open(csv_file, 'w', encoding='utf-8', newline='') as csv_file_obj:\n",
    "                writer = csv.DictWriter(csv_file_obj, fieldnames=['file_name', 'section_id', 'question', 'answer'])\n",
    "                writer.writeheader()\n",
    "                \n",
    "                # Get the file name from the full path\n",
    "                file_name = os.path.basename(data.get('file', 'unknown'))\n",
    "                \n",
    "                # Process each section in the results\n",
    "                for section in data.get('results', []):\n",
    "                    section_id = section.get('section_id', 'unknown')\n",
    "                    \n",
    "                    # Skip sections with errors\n",
    "                    if 'error' in section:\n",
    "                        continue\n",
    "                    \n",
    "                    # Process each Q&A pair\n",
    "                    for qa_pair in section.get('qa_pairs', []):\n",
    "                        question = qa_pair.get('question', '').replace('\\n', ' ').strip()\n",
    "                        answer = qa_pair.get('answer', '').replace('\\n', ' ').strip()\n",
    "                        \n",
    "                        # Write the row\n",
    "                        writer.writerow({\n",
    "                            'file_name': file_name,\n",
    "                            'section_id': section_id,\n",
    "                            'question': question,\n",
    "                            'answer': answer\n",
    "                        })\n",
    "            \n",
    "            csv_files.append(csv_file)\n",
    "            print(f\"Successfully converted to {csv_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {json_file} to CSV: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nProcessing summary:\")\n",
    "    print(f\"PDFs processed: {len(pdf_files)}\")\n",
    "    print(f\"JSON files generated: {len(json_files)}\")\n",
    "    print(f\"CSV files generated: {len(csv_files)}\")\n",
    "    \n",
    "    return {\n",
    "        \"pdf_files\": pdf_files,\n",
    "        \"json_files\": json_files,\n",
    "        \"csv_files\": csv_files\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the directory containing PDF files\n",
    "    pdf_directory = \"pdf_files\"  # Replace with your folder path\n",
    "    \n",
    "    # Set output directory (optional)\n",
    "    output_directory = \"qa_output\"  # Replace with your desired output folder\n",
    "    \n",
    "    # Process all PDFs and generate CSVs\n",
    "    results = process_pdfs_to_csv(\n",
    "        pdf_directory, \n",
    "        output_directory,\n",
    "        max_sections=None,  # Set to a number for debugging\n",
    "        use_cache=True      # Use caching to avoid repeated LLM calls\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53b4d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paste\n",
      "  Using cached Paste-3.10.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from paste) (72.1.0)\n",
      "Using cached Paste-3.10.1-py3-none-any.whl (289 kB)\n",
      "Installing collected packages: paste\n",
      "Successfully installed paste-3.10.1\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Using cached pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.6/5.6 MB 26.4 MB/s eta 0:00:00\n",
      "Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 38.9 MB/s eta 0:00:00\n",
      "Using cached pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "Installing collected packages: pypdfium2, Pillow, pdfminer.six, pdfplumber\n",
      "Successfully installed Pillow-11.2.1 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1\n",
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.42-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl.metadata (439 bytes)\n",
      "Collecting llama-index-cli<0.5,>=0.4.2 (from llama-index)\n",
      "  Downloading llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting llama-index-core<0.13,>=0.12.42 (from llama-index)\n",
      "  Downloading llama_index_core-0.12.42-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.4.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)\n",
      "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
      "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.65.4)\n",
      "Collecting aiohttp<4,>=3.8.6 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading aiohttp-3.12.12-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (2025.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (11.2.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (2.10.6)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (2.32.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.42->llama-index) (2.0.37)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.7.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.42->llama-index) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting llama-cloud==0.1.26 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.26-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-cloud==0.1.26->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5,>=0.4.0->llama-index)\n",
      "  Downloading openai-1.86.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting beautifulsoup4<5,>=4.12.3 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.3)\n",
      "Collecting pypdf<6,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
      "  Downloading pypdf-5.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.32-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading multidict-6.4.4-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting jinja2 (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index) (4.3.6)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama-index) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama-index) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama-index) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.42->llama-index) (0.14.0)\n",
      "Collecting llama-cloud-services>=0.6.32 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.32-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.42->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.42->llama-index) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.42->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.42->llama-index) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.42->llama-index) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.13,>=0.12.42->llama-index) (0.4.6)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting platformdirs (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.32->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.42->llama-index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.17.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama-index)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading llama_index-0.12.42-py3-none-any.whl (7.1 kB)\n",
      "Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_cli-0.4.3-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.12.42-py3-none-any.whl (7.7 MB)\n",
      "   ---------------------------------------- 0.0/7.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/7.7 MB 1.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.0/7.7 MB 1.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.3/7.7 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.8/7.7 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.4/7.7 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.7 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.1/7.7 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.7/7.7 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.7 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.7/7.7 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.2/7.7 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.2/7.7 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.2/7.7 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.5/7.7 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.0/7.7 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.8/7.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.7/7.7 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl (16 kB)\n",
      "Downloading llama_cloud-0.1.26-py3-none-any.whl (266 kB)\n",
      "Downloading llama_index_llms_openai-0.4.5-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl (3.4 kB)\n",
      "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
      "Downloading llama_index_readers_file-0.4.9-py3-none-any.whl (40 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.5 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.12.12-cp311-cp311-win_amd64.whl (451 kB)\n",
      "Downloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_parse-0.6.32-py3-none-any.whl (4.9 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.0/2.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading openai-1.86.0-py3-none-any.whl (730 kB)\n",
      "   ---------------------------------------- 0.0/730.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 730.3/730.3 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading pypdf-5.6.0-py3-none-any.whl (304 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl (893 kB)\n",
      "   ---------------------------------------- 0.0/893.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 893.9/893.9 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading llama_cloud_services-0.6.32-py3-none-any.whl (39 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.4-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: striprtf, filetype, dirtyjson, wrapt, tenacity, soupsieve, regex, python-dotenv, pypdf, propcache, platformdirs, networkx, mypy-extensions, multidict, marshmallow, MarkupSafe, joblib, griffe, frozenlist, attrs, aiosqlite, aiohappyeyeballs, yarl, typing-inspect, tiktoken, nltk, jinja2, deprecated, beautifulsoup4, aiosignal, openai, llama-cloud, dataclasses-json, banks, aiohttp, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.3.6\n",
      "    Uninstalling platformdirs-4.3.6:\n",
      "      Successfully uninstalled platformdirs-4.3.6\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.65.4\n",
      "    Uninstalling openai-1.65.4:\n",
      "      Successfully uninstalled openai-1.65.4\n",
      "Successfully installed MarkupSafe-3.0.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.12 aiosignal-1.3.2 aiosqlite-0.21.0 attrs-25.3.0 banks-2.1.2 beautifulsoup4-4.13.4 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 frozenlist-1.7.0 griffe-1.7.3 jinja2-3.1.6 joblib-1.5.1 llama-cloud-0.1.26 llama-cloud-services-0.6.32 llama-index-0.12.42 llama-index-agent-openai-0.4.11 llama-index-cli-0.4.3 llama-index-core-0.12.42 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.7 llama-index-llms-openai-0.4.5 llama-index-multi-modal-llms-openai-0.5.1 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.9 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.32 marshmallow-3.26.1 multidict-6.4.4 mypy-extensions-1.1.0 networkx-3.5 nltk-3.9.1 openai-1.86.0 platformdirs-4.3.8 propcache-0.3.2 pypdf-5.6.0 python-dotenv-1.1.0 regex-2024.11.6 soupsieve-2.7 striprtf-0.0.26 tenacity-9.1.2 tiktoken-0.9.0 typing-inspect-0.9.0 wrapt-1.17.2 yarl-1.20.1\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
      "     ---------------------------------------- 0.0/67.9 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 4.2/67.9 MB 28.1 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 4.5/67.9 MB 13.4 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 4.5/67.9 MB 13.4 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 4.5/67.9 MB 13.4 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 6.6/67.9 MB 6.5 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 10.5/67.9 MB 8.5 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 12.1/67.9 MB 8.1 MB/s eta 0:00:07\n",
      "     --------- ----------------------------- 17.3/67.9 MB 10.3 MB/s eta 0:00:05\n",
      "     ---------- ---------------------------- 18.9/67.9 MB 10.8 MB/s eta 0:00:05\n",
      "     ---------- ---------------------------- 18.9/67.9 MB 10.8 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 21.0/67.9 MB 9.3 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 24.9/67.9 MB 9.9 MB/s eta 0:00:05\n",
      "     ---------------- ---------------------- 28.0/67.9 MB 10.2 MB/s eta 0:00:04\n",
      "     ------------------ -------------------- 32.2/67.9 MB 10.9 MB/s eta 0:00:04\n",
      "     -------------------- ------------------ 35.4/67.9 MB 11.2 MB/s eta 0:00:03\n",
      "     ----------------------- --------------- 41.4/67.9 MB 12.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------ 45.9/67.9 MB 12.8 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 47.4/67.9 MB 12.5 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 48.2/67.9 MB 12.6 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 48.2/67.9 MB 12.6 MB/s eta 0:00:02\n",
      "     ----------------------------- --------- 50.6/67.9 MB 11.4 MB/s eta 0:00:02\n",
      "     ------------------------------- ------- 54.3/67.9 MB 11.8 MB/s eta 0:00:02\n",
      "     -------------------------------- ------ 56.9/67.9 MB 11.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 56.9/67.9 MB 11.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 59.2/67.9 MB 11.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 64.5/67.9 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.6/67.9 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 67.9/67.9 MB 11.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-cpp-python) (2.2.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\clayarnold\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "   Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "   exit code: 1\n",
      "  > [20 lines of output]\n",
      "      \u001b[32m*** \u001b[1mscikit-build-core 0.11.4\u001b[0m using \u001b[34mCMake 4.0.3\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "      \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "      2025-06-13 11:47:53,949 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "      loading initial cache file C:\\Users\\CLAYAR~1\\AppData\\Local\\Temp\\tmpvb_p83he\\build\\CMakeInit.txt\n",
      "      -- Building for: NMake Makefiles\n",
      "      CMake Error at CMakeLists.txt:3 (project):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      \u001b[31m\n",
      "      \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\n"
     ]
    }
   ],
   "source": [
    "!pip install paste\n",
    "!pip install pdfplumber\n",
    "!pip install llama-index\n",
    "!pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe735b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
